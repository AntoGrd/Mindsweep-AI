[
    {
        "title": "Getting Started with Couchbase: Installation and Setup Guide",
        "link": "https://www.kdnuggets.com/getting-started-with-couchbase-installation-and-setup-guide",
        "date": "2025-08-19T00:00:00+00:00",
        "content": "Image by Editor (Kanwal Mehreen) | Canva\n\n[Image: Getting Started with Couchbase] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Getting Started with Couchbase] https://www.kdnuggets.com/wp-content/uploads/couchbase.png\n\nCouchbase is a distributed NoSQL document-oriented database designed for interactive applications. It can be installed on Windows, macOS, and Linux, and it can also run inDockerif you prefer using containers. Once installed, Couchbase provides a web-based user interface to simplify the setup process, allowing you to create and manage your data clusters and buckets. This article will help you get started with Couchbase.\n\n#Prerequisites\n\nBefore starting the installation, make sure your system meets these requirements:\n\nOperating System: Couchbase Server can run on Windows, macOS, and LinuxHardware Requirements: At least 4 GB of RAM for testing, 8 GB or more for production, and a multi-core CPUDisk Space: At least 10 GB for Couchbase storage and indexesNetwork: Access to ports such as 8091 for the Couchbase Web Console and other service ports like 11210 for data\n\nDocker can also be used to run Couchbase for easier installation and isolation. For production environments, you’ll also need to manage Couchbase clusters across multiple nodes.\n\n#Downloading Couchbase\n\nTo get started, download the Couchbase Server package for your operating system:\n\nGo to theCouchbase Downloads PageChoose the appropriate version and platform (Windows, macOS, or Linux)Download the installation package\n\n#Installing Couchbase Server\n\n//Windows Installation\n\nRun the downloaded .exe fileFollow the on-screen instructions to complete the installationOnce installed, the Couchbase Server will start automatically\n\n//macOS Installation\n\nOpen the downloaded .dmg fileDrag the Couchbase Server icon into the Applications folderLaunch Couchbase Server from Applications\n\n//Linux Installation\n\nFor Debian/Ubuntu systems, use the following commands:\n\nsudo dpkg -i couchbase-server-enterprise_version.deb\r\nsudo systemctl start couchbase-server\n\n//Docker Installation\n\nCouchbase Server can also be deployed as a Docker container:\n\ndocker run -d --name couchbase -p 8091-8094:8091-8094 -p 11210:11210 couchbase:latest\n\nFor more configurations, visit the officialCouchbase Docker documentation.\n\n#Setting Up the Couchbase Web Console\n\nAfter installing Couchbase Server, you can use the Couchbase Web Console to manage and monitor your instance.\n\nOpen a web browserNavigate tohttp://localhost:8091, which is the default portThe setup wizard will guide you through the initial configuration steps\n\n#Creating a Cluster\n\nThe Couchbase Server requires at least one cluster for organizing data. During the setup process:\n\nChoose \"Create a New Cluster\"Name your cluster (e.g., MyCouchbaseCluster)Set a password for the Couchbase Admin accountAllocate available resources (RAM) for your cluster services\n\n#Adding a Bucket\n\nA bucket is a logical grouping of data, similar to a database or a collection. To create a new bucket:\n\nIn the Couchbase Web Console, go to the Buckets tabEnter a name for the bucket (e.g., UserData)Allocate memory to the bucket and configure other settings like replicas and eviction policiesClick Add Bucket to create it\n\nBuckets are Couchbase’s core units of storage and can be used to separate data by use case or application component.\n\n#Basic Configuration and Optimization\n\nAfter setting up the cluster and bucket, optimize the configuration for better performance:\n\nMemory Quota: Adjust memory allocation for Data, Index, and Query services based on workload requirementsAuto-Failover: Enable automatic failover to recover from node failures quicklyIndexing: Choose memory-optimized indexes for frequently queried data\n\n#Connecting to Couchbase via SDKs\n\nCouchbase provides Software Development Kits (SDKs) for multiple programming languages, including Java, Python, Node.js, and .NET. To connect to Couchbase from your application:\n\nInstall the Couchbase SDK for your language.For example, in Node.js:\n\nnpm install couchbase\n\nConnect to your cluster and perform CRUD (Create, Read, Update, Delete) operations.Here is a basic example:\n\nasync function run() {\r\n    // Insert a document\r\n    await collection.upsert(\"user::123\", { name: \"John Doe\", age: 29 });\r\n\r\n    // Retrieve the document\r\n    const result = await collection.get(\"user::123\");\r\n    console.log(\"User:\", result.content);\r\n}\r\n\r\nrun().catch((err) => console.error(\"Error:\", err));\n\nEach SDK has its own detailed documentation for deeper functionality, which you can find on theCouchbase SDK page.\n\n#Using the Couchbase Command Line Interface\n\nCouchbase also includes a Command Line Interface (CLI) for managing clusters. The general syntax of acouchbase-clicommand is as follows:\n\ncouchbase-cli-c:-u-p[options]\n\n#Common Couchbase CLI Commands\n\nHere are some of the most commonly used commands:\n\nCluster Initialization: Initializes a new Couchbase cluster\n\ncouchbase-cli cluster-init -c localhost:8091 -u Administrator -p password \\\r\n    --cluster-username Administrator --cluster-password password \\\r\n    --services data,index,query\n\nBucket Creation: Creates a new bucket for storing data\n\ncouchbase-cli bucket-create -c localhost:8091 -u Administrator -p password \\\r\n    --bucket testBucket --bucket-type couchbase --bucket-ramsize 100\n\nAdding a Node: Adds a new node to the cluster\n\ncouchbase-cli server-add -c localhost:8091 -u Administrator -p password \\\r\n    --server-add--server-add-username Administrator \\\r\n    --server-add-password password\n\nRebalancing the Cluster: Rebalances the cluster after adding or removing nodes\n\ncouchbase-cli rebalance -c localhost:8091 -u Administrator -p password\n\n#Verifying the Setup\n\nTo verify that your Couchbase Server setup is working:\n\nWeb Console: Check the Couchbase Web Console for the health of your cluster and bucketsMetrics: Monitor server and cluster health using the built-in metrics in CouchbaseSample Query: Run a sample query usingN1QLin the Query Editor tab of the Web Console\n\n#Conclusion\n\nCouchbase is a powerful NoSQL database built for modern applications. Its straightforward installation on Windows, macOS, Linux, and Docker allows for quick setup. The web console simplifies management, while clusters and buckets provide robust data organization. By tuning memory and indexing settings, you can optimize performance for speed and efficiency. Furthermore, Couchbase's SDKs allow for seamless integration with various programming languages, and the CLI provides a robust toolset for command-line management.\n\nJayita Gulatiis a machine learning enthusiast and technical writer driven by her passion for building machine learning models. She holds a Master's degree in Computer Science from the University of Liverpool.\n\nMore On This Topic\n\nGetting Started with Llamafactory: Installation and Setup GuideGetting Started with Redis: Installation and Setup GuideGetting Started with MongoDB: Installation and Setup GuideGetting Started with Cassandra: Installation and Setup GuideGetting Started with Neo4j: Installation and Setup GuideBeginner’s Guide to Gemini CLI: Install, Setup, and Use It Like a Pro\n\n"
    },
    {
        "title": "7 Surprisingly Useful Python Scripts You’ll Use Every Week",
        "link": "https://www.kdnuggets.com/7-surprisingly-useful-python-scripts-youll-use-every-week",
        "date": "2025-08-19T00:00:00+00:00",
        "content": "Image by Author | Ideogram\n\n[Image: 7 Surprisingly Useful Python Scripts You’ll Use Every Week] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: 7 Surprisingly Useful Python Scripts You’ll Use Every Week] https://www.kdnuggets.com/wp-content/uploads/a-comic-book-style-illustration-depictin_If95sACnQzar_CT7wKDu7w_kC2tQXubSNak80I-TErcNw.jpeg\n\n#Introduction\n\nPythonisn’t just for data science, building apps, or making games. I was teaching my younger brother, Qasim, Python, and I realized he connected a lot more when I showed him practical scripts that could automate boring, repetitive tasks we often deal with every week. One thing I’ve observed in life is that a bunch of small, unproductive, time-consuming tasks can seriously drain your energy. You end up with little motivation or focus for the things that actually matter. So, I’m here to help you out with some of the most common tasks that you encounter almost every week and how they can be automated with Python easily, saving you tons of time and energy.\n\nI’ve kept the scripts clean and simple, but you can add complexity to them as per your requirements.So, let's get started.\n\n#1. Automated File Organizer by Extension\n\nTo be honest, sometimes when I’m working, my Downloads folder becomes an absolute mess. I’ll share a simple Python script that will loop through the files in your target directory and arrange them (e.g., images, documents, videos) by extension usingosandshutil. Let’s have a look:\n\nimport os\r\nimport shutil\r\nfrom pathlib import Path\r\n\r\n# Folder to organize (e.g., Downloads)\r\nbase_folder = Path.home() / \"Downloads\"\r\n\r\n# Define folders for each extension\r\nfolders = {\r\n    \"images\": [\"jpg\", \"png\", \"gif\", \"bmp\"],\r\n    \"documents\": [\"txt\", \"pdf\", \"docx\"],\r\n    \"archives\": [\"zip\", \"rar\", \"tar\", \"gz\"],\r\n    \"audio\": [\"mp3\", \"wav\"],\r\n    # add more categories as needed\r\n}\r\n\r\n# Iterate over files in base_folder\r\nfor item in base_folder.iterdir():\r\n    if item.is_file():\r\n        ext = item.suffix.lower().lstrip('.')\r\n        moved = False\r\n        # Determine which category folder to use\r\n        for folder, ext_list in folders.items():\r\n            if ext in ext_list:\r\n                dest_dir = base_folder / folder\r\n                dest_dir.mkdir(exist_ok=True)\r\n                item.rename(dest_dir / item.name)\r\n                moved = True\r\n                break\r\n        # If extension didn’t match any category, move to \"others\"\r\n        if not moved:\r\n            dest_dir = base_folder / \"others\"\r\n            dest_dir.mkdir(exist_ok=True)\r\n            item.rename(dest_dir / item.name)\n\n#2. System Resource Monitor with Alerts\n\nIf you’re someone like me who runs multiple tabs, apps, and scripts together, it’s easy to lose track of your system performance. This script, using thepsutillibrary, helps you monitor your CPU and RAM usage. You can even set alerts if usage crosses a certain threshold.\n\nimport psutil\r\nimport time\r\n\r\nCPU_LIMIT = 80  # in percentage\r\nMEMORY_LIMIT = 80  # in percentage\r\n\r\nwhile True:\r\n    cpu = psutil.cpu_percent(interval=1)\r\n    memory = psutil.virtual_memory().percent\r\n\r\n    print(f\"CPU: {cpu}%, Memory: {memory}%\")\r\n\r\n    if cpu > CPU_LIMIT:\r\n        print(\"⚠️ CPU usage is high!\")\r\n\r\n    if memory > MEMORY_LIMIT:\r\n        print(\"⚠️ Memory usage is high!\")\r\n\r\n    time.sleep(5)\n\nRun it for a while, and you’ll start noticing how things spike when you're editing videos or running a heavy script.\n\n#3. Automated Email Reporter\n\nEmail eats up way more time than we think. Whether it’s replying to updates, sending out reminders, following up, or just keeping people in the loop, we often spend hours every week doing things that can (and should) be automated. And if you’re anything like me, you probably overthink what to write in those regular update emails, end up procrastinating, and then rush through it later. This simple script can be really useful, and here’s how you can set it up:\n\nimport smtplib\r\nfrom email.mime.text import MIMEText\r\n\r\nsender = \"youremail@example.com\"\r\nreceiver = \"receiver@example.com\"\r\nsubject = \"Daily Report\"\r\nbody = \"This is your automated email for today.\"\r\n\r\nmsg = MIMEText(body)\r\nmsg[\"Subject\"] = subject\r\nmsg[\"From\"] = sender\r\nmsg[\"To\"] = receiver\r\n\r\n# For security, use environment variables or getpass instead of hardcoding.\r\npassword = \"your_password\" \r\n\r\nwith smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\r\n    server.login(\"youremail@example.com\", password)\r\n    server.send_message(msg)\r\n\r\nprint(\"Email sent!\")\n\n⚠️Heads up:If you're using Gmail, make sure you've enabled less secure app access or set up an app-specific password if you have 2FA enabled.\n\n#4. Desktop Notifications\n\nI actually started using this script to help me stick to thePomodoro Technique(25 minutes of deep focus followed by a 5-minute break). And honestly, it worked wonders for my concentration and energy levels throughout the day. It’s great if you want something that pops up a little reminder on your screen without switching apps or setting alarms. You can use it for anything like stretch reminders, hydration alerts, or even to remind yourself to stop doomscrolling.Since I’m on macOS, I’m using the built-inosascriptcommand to trigger native system notifications. But if you’re on Windows or Linux, you can use theplyerlibrary for similar functionality.\n\nHere's how I set it up for my own workflow:\n\nimport time\r\nimport os\r\n\r\ndef notify(title, message):\r\n    os.system(f'''\r\n        osascript -e 'display notification \"{message}\" with title \"{title}\"'\r\n    ''')\r\n\r\n# Pomodoro session settings\r\nwork_duration = 25 * 60  # 25 minutes\r\nbreak_duration = 5 * 60  # 5 minutes\r\ncycles = 4  # Number of Pomodoro sessions\r\n\r\nfor i in range(cycles):\r\n    # Work phase\r\n    notify(f\"Pomodoro {i + 1} - Focus Time\", \"Time to focus! Work for 25 minutes.\")\r\n    time.sleep(work_duration)\r\n\r\n    # Break phase\r\n    notify(\"Break Time\", \"Nice work! Take a 5-minute break.\")\r\n    time.sleep(break_duration)\r\n\r\n# Final notification\r\nnotify(\"All Done!\", \"You’ve completed all your Pomodoros 🎉\")\n\n#5. Password Generator and Manager\n\nIf you’re still using the same password everywhere (please don’t), this is for you. It generates a secure password and stores it safely (you can later encrypt the storage part too).\n\nimport random\r\nimport string\r\n\r\ndef generate_password(length=12):\r\n    chars = string.ascii_letters + string.digits + string.punctuation\r\n    password = \"\".join(random.choice(chars) for _ in range(length))\r\n    return password\r\n\r\nnew_password = generate_password()\r\nprint(\"Generated Password:\", new_password)\r\n\r\n# Save to file (simple way)\r\nwith open(\"passwords.txt\", \"a\") as file:\r\n    file.write(f\"MySite: {new_password}\\n\")\n\nFor more security, look into encrypting the file withcryptographyor storing it in a secure vault likekeyring.\n\n#6. Search for Text in Multiple Files\n\nWhen I’m working on writing material, I usually have tons of raw ideas scattered across various files in different folders. Sometimes I remember writing a brilliant analogy or a code snippet weeks ago... but I have no idea where I saved it. This little script has saved me countless hours. Instead of manually opening every file to search for a phrase like “machine learning” or “vector search,” I just run this Python script and let it scan everything for me.\n\nimport os\r\n\r\nsearch_dir = \"your_directory\"  # Replace with the path to your notes\r\nsearch_term = \"machine learning\"\r\n\r\nfor root, dirs, files in os.walk(search_dir):\r\n    for file in files:\r\n        if file.endswith(\".txt\") or file.endswith(\".md\"):\r\n            file_path = os.path.join(root, file)\r\n            try:\r\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\r\n                    content = f.read()\r\n                    if search_term.lower() in content.lower():\r\n                        print(f\"✅ Found in: {file_path}\")\r\n            except Exception as e:\r\n                print(f\"❌ Skipped {file_path} (error reading file)\")\n\nThis is a simple version that works great for plain text files. Since I also work with.docx,.pptx, and.pdffiles, I use a slightly more advanced version that supports those formats too. You can easily extend this script using libraries likepython-docx,python-pptx, andpdfplumberto make it a mini search engine for your entire workspace.\n\n#7. Finding Internships and Scholarships on Twitter\n\nTwitter/X can be a goldmine if you know how to use it smartly. When I was actively searching for master’s and PhD scholarships, I realized that you don’t just need to be qualified, but you also need to be quick and aware. Many great opportunities pop up on Twitter (yes, seriously), but they vanish fast if you’re not watching closely. There are two great ways to do this in Python: using either thesnscrapelibrary or Twitter’s official API. If you want more control (like filtering by language, excluding retweets, etc.), you can use Twitter’s official API v2. Even with the free version, you get limited access to recent tweets. For this script, we will use therequestslibrary to interact with the API andpandasto organize the results.\n\nYou’ll need a developer account and a Bearer Token (from the Twitter/X Developer Portal).\n\nimport requests\r\nimport pandas as pd\r\n\r\nBEARER_TOKEN = 'YOUR_TWITTER_BEARER_TOKEN'  # Replace this with your token\r\n\r\nheaders = {\r\n    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\r\n}\r\n\r\nsearch_url = \"https://api.twitter.com/2/tweets/search/recent\"\r\n\r\n# Keywords that usually show up in academic opportunities\r\nquery = (\r\n    '(phd OR \"phd position\" OR \"master position\" OR \"fully funded\") '\r\n    '(\"apply now\" OR \"open position\" OR \"graduate position\") '\r\n    '-is:retweet lang:en'\r\n)\r\n\r\nparams = {\r\n    'query': query,\r\n    'max_results': 10,  # Max is 100 for recent search\r\n    'tweet.fields': 'author_id,created_at,text',\r\n    'expansions': 'author_id',\r\n    'user.fields': 'username,name',\r\n}\r\n\r\ndef get_tweets():\r\n    response = requests.get(search_url, headers=headers, params=params)\r\n    if response.status_code != 200:\r\n        raise Exception(f\"Request failed: {response.status_code}, {response.text}\")\r\n    return response.json()\r\n\r\ndef extract_tweet_info(data):\r\n    users = {u['id']: u for u in data.get('includes', {}).get('users', [])}\r\n    tweets_info = []\r\n\r\n    for tweet in data.get('data', []):\r\n        user = users.get(tweet['author_id'], {})\r\n        tweet_url = f\"https://twitter.com/{user.get('username')}/status/{tweet['id']}\"\r\n\r\n        tweets_info.append({\r\n            'Date': tweet['created_at'],\r\n            'Username': user.get('username'),\r\n            'Name': user.get('name'),\r\n            'Text': tweet['text'],\r\n            'Tweet_URL': tweet_url\r\n        })\r\n\r\n    return pd.DataFrame(tweets_info)\r\n\r\nif __name__ == \"__main__\":\r\n    data = get_tweets()\r\n    df = extract_tweet_info(data)\r\n    print(df[['Date', 'Username', 'Text', 'Tweet_URL']].head())\r\n    df.to_csv('phd_masters_positions_twitter.csv', index=False)\n\nYou can run this weekly and save the results to a CSV or even email them to yourself.\n\n#Wrapping Up\n\nHonestly, I didn’t start using Python for automation, but over time I realized how much time goes into small, repetitive stuff that quietly eats away at my focus. These scripts might seem basic, but they genuinely help free up time and headspace.\n\nYou don’t need to automate everything. Just pick the one or two tasks that you do the most often and build from there. I’ve found that once you start with simple automation, you begin spotting more ways to make life a bit easier. That’s where it really starts to click.\n\nLet me know if you end up using any of these or build your own versions. I would love to see what you come up with.\n\nKanwal Mehreenis a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook \"Maximizing Productivity with ChatGPT\". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields.\n\nMore On This Topic\n\n5 Useful Python Scripts for Busy Data Scientists5 Genuinely Useful Bash Scripts for Data Science10 FREE AI Tools That’ll Save You 10+ Hours a WeekBack to Basics Week 1: Python Programming & Data Science FoundationsLearn Python and get Certified as a Data Analyst for Free this Week!This Week in AI, August 7: Generative AI Comes to Jupyter & Stack…\n\n"
    },
    {
        "title": "How to Create Powerful LLM Applications with Context Engineering",
        "link": "https://towardsdatascience.com/how-to-create-powerful-llm-applications-with-context-engineering/",
        "date": "2025-08-18T13:13:36-05:00",
        "content": "Context\nengineering is a powerful concept you can utilize to increase the effectiveness of your LLM applications. In this article, I elaborate on context engineering techniques and how to succeed with AI applications utilizing effective context management. Thus, if you are working on AI applications utilizing LLMs, I highly recommend reading the full contents of the article.\nI first wrote about the topic of context engineering\nin my article:\nHow You Can Enhance LLMs with Context Engineering\n, where I discussed some context engineering techniques and important notes. In this article, I expand on the topic by discussing more context engineering techniques and how to do evaluations on your context management.\nIn this article, I discuss how you can utilize context engineering to increase the efficiency of your LLMs. Image by ChatGPT.\nIf you haven’t read it already, I recommend you first read\nmy initial article on context engineering\n, or you can read about\nensuring reliability in LLM applications.\nTable of Contents\nMotivation\nContext engineering techniques\nPrompt structuring\nContext window management\nKeyword search (vs RAG)\nEvaluation\nConclusion\nMotivation\nMy motivation for writing this article is similar to my\nlast article on context engineering\n. LLMs have become incredibly important in a lot of applications since the release of ChatGPT in 2022. However, LLMs are often not utilized to their full potential due to poor context management. Proper context management requires context engineering skills and techniques, which is what I’ll discuss in this article. Thus, if you are working on any applications utilizing LLMs, I highly recommend taking notes from this article and integrating it into your own application.\nContext engineering techniques\nIn my last article, I discussed context engineering techniques such as:\nZero/few-shot prompting\nRAG\nTools (MCP)\nI’ll now elaborate on more techniques that are important to proper context management.\nPrompt structuring\nWith prompt structuring, I’m referring to how your prompt is organized. A messy prompt will, for example, contain all the text without line breaks, repetitive instructions, and unclear sectioning. Check out the example below for a properly structured prompt, vs a messy prompt:\n#\nunstructured prompt. No line breaks, repetitive instructions, unclear sectioning\n\"You are an AI assistant specializing in question answering. You answer the users queries in a helpful, concise manner, always trying to be helpful. You respond concisely, but also avoid single-word answers.\"\n#\nstructured prompt:\n\"\"\"\n##\nRole\nYou are an\n**\nAI assistant specializing in question answering\n**\n.\n##\nObjectives\n1.\nAnswer user queries in a\n**\nhelpful\n**\nand\n**\nconcise\n**\nmanner.\n2.\nAlways prioritize\n**\nusefulness\n**\nin responses.\n##\nStyle Guidelines\n-\n**\nConcise, but not overly brief\n**\n: Avoid single-word answers.\n-\n**\nClarity first\n**\n: Keep responses straightforward and easy to understand.\n-\n**\nBalanced tone\n**\n: Professional, helpful, and approachable.\n##\nResponse Rules\n-\nProvide\n**\ncomplete answers\n**\nthat cover the essential information.\n-\nAvoid unnecessary elaboration or filler text.\n-\nEnsure answers are\n**\ndirectly relevant\n**\nto the user’s question.  \n\"\"\"\nPrompt structuring is important for two reasons.\nIt makes the instructions clearer to the AI\nIt increases (human) readability of the prompt, which helps you detect potential issues with your prompt, avoid repetitive instructions, etc\nYou should always try to avoid repetitive instructions. To avoid this, I recommend feeding your prompt into another LLM and asking for feedback. You’ll typically receive back a much cleaner prompt, with clearer instructions. Anthropic also has\na prompt generator\nin their dashboard, and there are also a lot of other tools out there to improve your prompts.\nContext window management\nTwo main points for context management. Keep the context short, and if the context gets too long, you can utilize context compression by summarizing. Image by ChatGPT.\nAnother important point to keep in mind is context window management. With this, I am referring to the amount of tokens you are feeding into your LLM.\nIt’s important to remember that while recent LLMs have super-long context windows (for ex\nample,\nLlama 4 Scout with a 10M context window\n), they are not necessarily able to utilize all of those tokens. You can, for example,\nread this article\n, highlighting how LLMs perform worse with more input tokens, even if the difficulty of the problem stays the same.\nIt’s thus important to properly manage your context window. I recommend focusing on two points:\nKeep the prompt as short as possible, while including all relevant information. Look through the prompt and determine if there is any irrelevant text there. If so, removing it will likely increase LLM performance\nYou might be experiencing problems where the LLM runs out of context window. Either because of the hard context size limit, or because too many input tokens make the LLM slow to respond. In these cases, you should consider\ncontext compression\nFor point one, it’s important to note that this irrelevant information is often not a part of your static system prompt, but rather the dynamic information you are feeding into the context. For example, if you are fetching information using RAG, you should consider excluding chunks that have similarity below a specific threshold. This threshold will vary from application to application, though empirical reasoning here typically works well.\nContext compression is another powerful technique you can use to properly manage the context of your LLM. Context compression is typically done by prompting another LLM to summarize part of your context. This way, you can contain the same information using fewer tokens. This approach is, for example, used to handle the context window of agents, which can quickly expand as the agent performs more actions.\nKeyword search (vs RAG)\nThis image shows RAG architecture from https://github.com/infiniflow/ragflow (Apache 2 license). You can improve the RAG flow by implementing contextual retrieval.\nAnother topic I think is worth highlighting is to utilize keyword search, in addition to retrieval augmented generation (RAG). In most AI applications, the focus is on RAG, considering it can fetch information based on semantic similarity.\nSemantic similarity is super powerful because in a lot of cases, the user doesn’t know the exact wording of what they are looking for. Searching for semantic similarity thus works very well. However, in a lot of cases, keyword search will also work super well. I thus recommend integrating an option to fetch documents using some sort of keyword search, in addition to your RAG. The keyword search will, in some scenarios, retrieve more relevant documents than RAG is able to.\nAnthropic highlighted this approach\nwith their article on Contextual Retrieval from September 2024.\nIn this article, they show you how you can utilize BM25 to fetch relevant information in your RAG system effectively.\nEvaluation\nEvaluation is an important part of any machine-learning system. If you don’t know how well your LLMs are performing, it’s hard to improve your system.\nThe first step to evaluation is observability. I thus recommend implementing prompt management software. You can find a series of such tools on\nthis GitHub page.\nOne way to evaluate your context management is to perform A/B testing. You simply run two different versions of a prompt, using different context management techniques. Then you can, for example, gather user feedback to determine which approach works better. Another way to test it is to prompt an LLM with the problem you are trying to solve (for example, RAG) and the context you are using to answer the RAG query. The LLM can then provide you with feedback on how to improve your context management.\nFurthermore, an underrated approach to improving the quality of the contexts is to manually inspect them. I believe a lot of engineers working with LLMs spend too little time on manual inspections, and analyzing the input tokens fed into LLMs falls under this category. I thus recommend setting aside time to go through a series of different contexts that are fed into your LLM, to determine how you can improve. Manual inspection provides you with the opportunity to properly understand the data you are working with and what you are feeding into your LLMs.\nConclusion\nIn this article, I have elaborated on the topic of context engineering. Working on context engineering is a powerful approach to improving your LLM application. There are a series of techniques you can utilize to better manage the context of your LLMs, for example, improving your prompt structure, proper context window management, utilizing keyword search, and context compression. Furthermore, I also discussed evaluating the contexts.\nFind me on socials:\nGet in touch\nLinkedIn\nX / Twitter\nMedium"
    },
    {
        "title": "Writing Your First GPU Kernel in Python with Numba and CUDA",
        "link": "https://www.kdnuggets.com/writing-your-first-gpu-kernel-in-python-with-numba-and-cuda",
        "date": "2025-08-18T00:00:00+00:00",
        "content": "Image by Author | Ideogram\n\n[Image: Writing Your First GPU Kernel in Python with Numba and CUDA] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Writing Your First GPU Kernel in Python with Numba and CUDA] https://www.kdnuggets.com/wp-content/uploads/a-clean-vector-illustration-diagram-comp_Tz_o2ZHbQQanglazWiH3cw_IWJ0NepSQi6DkXbE8nWVYA.jpeg\n\nGPUs are great for tasks where you need to do the same operation across different pieces of data. This is known as theSingle Instruction, Multiple Data (SIMD)approach. Unlike CPUs, which only have a few powerful cores, GPUs have thousands of smaller ones that can run these repetitive operations all at once. You will see this pattern a lot in machine learning, for example when adding or multiplying large vectors, because each calculation is independent. This is the ideal scenario for using GPUs to speed up tasks with parallelism.\n\nNVIDIA createdCUDAas a way for developers to write programs that run on the GPU instead of the CPU. It’s based on C and lets you write special functions called kernels that can run many operations at the same time. The problem is that writing CUDA in C or C++ isn’t exactly beginner-friendly. You have to deal with things like manual memory allocation, thread coordination, and understanding how the GPU works at a low level. This can be overwhelming especially if you're used to writing code in Python.\n\nThis is whereNumbacan help you.It allows writing CUDA kernels with Python using the LLVM (Low Level Virtual Machine) compiler infrastructure to directly compile your Python code to CUDA-compatible kernels.With just-in-time (JIT) compilation, you can annotate your functions with a decorator, and Numba handles everything else for you.\n\nIn this article, we will use a common example of vector addition, and convert simple CPU code to a CUDA kernel with Numba. Vector addition is an ideal example of parallelism, as addition across a single index is independent of other indices. This is the perfect SIMD scenario so all indices can be added simultaneously to complete vector addition in one operation.\n\nNote that you will require a CUDA GPU to follow this article. You can useColab'sfree T4 GPU or a local GPU with NVIDIA toolkit and NVCC installed.\n\n#Setting Up the Environment and Installing Numba\n\nNumba is available as a Python package, and you can install it with pip. Moreover, we will usenumpyfor vector operations. Set up the Python environment using the following commands:\n\npython3 -m venv venv\r\nsource venv/bin/activate\r\npip install numba-cuda numpy\n\n#Vector Addition on the CPU\n\nLet’s take a simple example of vector addition. For two given vectors, we add the corresponding values from each index to get the final value. We will use numpy to generate random float32 vectors and generate the final output using a for loop.\n\nimport numpy as np \r\n\r\nN = 10_000_000 # 10 million elements \r\na = np.random.rand(N).astype(np.float32) \r\nb = np.random.rand(N).astype(np.float32) \r\nc = np.zeros_like(a) # Output array \r\n\r\ndef vector_add_cpu(a, b, c): \r\n    \"\"\"Add two vectors on CPU\"\"\" \r\n    for i in range(len(a)): \r\n        c[i] = a[i] + b[i]\n\nHere is a breakdown of the code:\n\nInitialize two vectors each with 10 million random floating-point numbersWe also create an empty vectorcto store the resultThevector_add_cpufunction simply loops through each index and adds the elements fromaandb, storing the result inc\n\nThis is aserial operation; each addition happens one after another. While this works fine, it's not the most efficient approach, especially for large datasets. Since each addition is independent of the others, this is a perfect candidate for parallel execution on a GPU.\n\nIn the next section, you will see how to convert this same operation to run on the GPU using Numba. By distributing each element-wise addition across thousands of GPU threads, we can complete the task significantly faster.\n\n#Vector Addition on the GPU with Numba\n\nYou will now use Numba to define a Python function that can run on CUDA, and execute it within Python. We are doing the same vector addition operation but now it can run in parallel for each index of the numpy array, leading to faster execution.\n\nHere is the code for writing the kernel:\n\nfrom numba import config\r\n\r\n# Required for newer CUDA versions to enable linking tools. \r\n# Prevents CUDA toolkit and NVCC version mismatches.\r\nconfig.CUDA_ENABLE_PYNVJITLINK = 1\r\n\r\nfrom numba import cuda, float32\r\n\r\n@cuda.jit\r\ndef vector_add_gpu(a, b, c):\r\n\t\"\"\"Add two vectors using CUDA kernel\"\"\"\r\n\t# Thread ID in the current block\r\n\ttx = cuda.threadIdx.x\r\n\t# Block ID in the grid\r\n\tbx = cuda.blockIdx.x\r\n\t# Block width (number of threads per block)\r\n\tbw = cuda.blockDim.x\r\n\r\n\t# Calculate the unique thread position\r\n\tposition = tx + bx * bw\r\n\r\n\t# Make sure we don't go out of bounds\r\n\tif position < len(a):\r\n    \t    c[position] = a[position] + b[position]\r\n\r\ndef gpu_add(a, b, c):\r\n\t# Define the grid and block dimensions\r\n\tthreads_per_block = 256\r\n\tblocks_per_grid = (N + threads_per_block - 1) // threads_per_block\r\n\r\n\t# Copy data to the device\r\n\td_a = cuda.to_device(a)\r\n\td_b = cuda.to_device(b)\r\n\td_c = cuda.to_device(c)\r\n\r\n\t# Launch the kernel\r\n\tvector_add_gpu[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\r\n\r\n\t# Copy the result back to the host\r\n\td_c.copy_to_host(c)\r\n\r\ndef time_gpu():\r\n\tc_gpu = np.zeros_like(a)\r\n\tgpu_add(a, b, c_gpu)\r\n\treturn c_gpu\n\nLet’s break down what is happening above.\n\n//Understanding the GPU Function\n\nThe@cuda.jitdecorator tells Numba to treat the following function as a CUDA kernel; a special function that will run in parallel across many threads on the GPU. At runtime, Numba will compile this function to CUDA-compatible code and handle the C-API transpilation for you.\n\n@cuda.jit\r\ndef vector_add_gpu(a, b, c):\r\n\t...\n\nThis function will run on thousands of threads at the same time. But we need a way to figure out which part of the data each thread should work on. That’s what the next few lines do:\n\ntxis the thread’s ID within its blockbxis the block’s ID within the gridbwis how many threads there are in a block\n\nWe combine these tocalculate a unique position, which tells each thread which element of the arrays it should add. Note that the threads and blocks might not always provide a valid index, as they operate in powers of 2. This may lead to invalid indices when the vector length is not conforming to the underlying architecture. Therefore, we add a guard condition to validate the index, before we perform the vector addition. This prevents any out-of-bound runtime error when accessing the array.\n\nOnce we know the unique position, we can now add the values just like we did for the CPU implementation. The following line will match the CPU implementation:\n\nc[position] = a[position] + b[position]\n\n//Launching the Kernel\n\nThegpu_addfunction sets things up:\n\nIt defines how many threads and blocks to use. You can experiment with different values of block and thread sizes, and print the corresponding values in the GPU kernel. This can help you understand how underlying GPU indexing works.It copies the input arrays (a,b, andc) from the CPU memory to the GPU memory, so the vectors are accessible in the GPU RAM.It runs the GPU kernel withvector_add_gpu[blocks_per_grid, threads_per_block].Finally, it copies the result back from the GPU into thecarray, so we can access the values on the CPU.\n\n#Comparing the Implementations and Potential Speedup\n\nNow that we have both the CPU and GPU versions of vector addition, it’s time to see how they compare. It is important to verify the results and the execution boost we can get with CUDA parallelism.\n\nimport timeit\r\n\r\nc_cpu = time_cpu()\r\nc_gpu = time_gpu()\r\n\r\nprint(\"Results match:\", np.allclose(c_cpu, c_gpu))\r\n\r\ncpu_time = timeit.timeit(\"time_cpu()\", globals=globals(), number=3) / 3\r\nprint(f\"CPU implementation: {cpu_time:.6f} seconds\")\r\n\r\ngpu_time = timeit.timeit(\"time_gpu()\", globals=globals(), number=3) / 3\r\nprint(f\"GPU implementation: {gpu_time:.6f} seconds\")\r\n\r\nspeedup = cpu_time / gpu_time\r\nprint(f\"GPU speedup: {speedup:.2f}x\")\n\nFirst, we run both implementations and check if their results match. This is important to make sure our GPU code is working correctly and the output should be the same as the CPU’s.\n\nNext, we use Python’s built-intimeitmodule to measure how long each version takes. We run each function a few times and take the average to get a reliable timing. Finally, we calculate how many times faster the GPU version is compared to the CPU. You should see a big difference because the GPU can do many operations at once, while the CPU handles them one at a time in a loop.\n\nHere is the expected output on NVIDIA’s T4 GPU on Colab. Note that the exact speedup can differ based on CUDA versions and the underlying hardware.\n\nResults match: True\r\nCPU implementation: 4.033822 seconds\r\nGPU implementation: 0.047736 secondsGPU speedup: 84.50x\n\nThis simple test helps demonstrate the power of GPU acceleration and why it’s so useful for tasks involving large amounts of data and parallel work.\n\n#Wrapping Up\n\nAnd that is it. You have now written your first CUDA kernel with Numba, without actually writing any C or CUDA code. Numba allows a simple interface for using the GPU through Python, and it makes it much simpler for Python engineers to get started with CUDA programming.\n\nYou can now use the same template to write advanced CUDA algorithms, which are prevalent in machine learning and deep learning. If you find a problem following the SIMD paradigm, it is always a good idea to use GPU to improve execution.\n\nThe complete code is available on Colab notebook that you can accesshere. Feel free to test it out and make simple changes to get a better understanding of how CUDA indexing and execution works internally.\n\nKanwal Mehreenis a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook \"Maximizing Productivity with ChatGPT\". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields.\n\nMore On This Topic\n\nBuilding a GPU Machine vs. Using the GPU CloudDensity Kernel Depth for Outlier Detection in Functional DataMastering GPUs: A Beginner's Guide to GPU-Accelerated DataFrames in PythonGenerative AI Playground: LLMs with Camel-5b and Open LLaMA 3B on…Generative AI Playground: Text-to-Image Stable Diffusion with…Using RAPIDS cuDF to Leverage GPU in Feature Engineering\n\n"
    },
    {
        "title": "If You’re Trying to Get Into AI, This Is What You Need to Do",
        "link": "https://www.kdnuggets.com/if-youre-trying-to-get-into-ai-this-is-what-you-need-to-do",
        "date": "2025-08-18T00:00:00+00:00",
        "content": "Image by Editor | ChatGPT\n\n[Image: Get Into AI] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Get Into AI] https://www.kdnuggets.com/wp-content/uploads/kdn-arya-trying-get-into-do-this.png\n\n#Introduction\n\nFor a young person coming out of high school or college, or even for parents of young children, it can be quite overwhelming to choose a career. For an older professional trying to transition into the AI landscape, it can be even worse. Adding AI on top of that can make it even more frustrating. Many are worried that they will lose their jobs to AI, and they are within their rights to have these thoughts. However, rather than fighting the movement of AI, why not figure out ways to work with it?\n\nAt the 8th FII Summitpanel discussion, Prem Akkaraju, CEO ofStability AI, discussed three important lessons he would give to young people about the next steps they should be making in their academic or professional careers.\n\n#Lesson 1: Don't Waste Your Time Learning How to Code\n\nThis may be a controversial one, as the AI market at the moment is in need of talent to meet soaring demands. However, if you’re already on your journey, I am not saying to drop your tools.\n\nThis point made by Akkaraju was solely aimed at people starting their careers or education right now. He says that the new coding language will be English. As it is already a universal language, more and more organizations will release AI tools in English to cater to the majority. Therefore, that should be your priority to segue into learning everything you can about AI.\n\nHowever, Richard Socher, CEO & Founder ofyou.comand Co-Founder & Managing Director of AIX Ventures, disagrees with this and says that learning how to code will allow you to truly understand the story behind AI. It becomes less like magic and more like something you can then take and modify to build a career or an organization.\n\n#Lesson 2: Learn About AI and AI Modalities\n\nThe best way to stay ahead of the curve is by learning everything you possibly can. Akkaraju urges young people to learn everything about AI as fast as possible, from its history and modalities to its potential and limitations.\n\nSocher states that in the coming five to ten years, there will be more multi-modal models, conversations over images, as well as video, sound, programming enhancements, and more. If you’re looking to enter the AI space, you do not have to learn all of them and overcrowd your brain; however, do not fixate on and limit yourself to just one. The best approach is to gain a general knowledge of all of them and, based on your career goals and passion, choose one that interests you and dive deep into it.\n\n#Lesson 3: Your Passion + AI\n\nAI will surround us and will be implemented in every sector. Therefore, once you have learned about AI and its potential, you should start thinking about how this aligns with your passion. For example, if you’re a fine arts artist, you should be looking into content generation and its future, and use this to figure out ways you can monetize and fulfill your passion.\n\nApplying your computer science and AI knowledge to your passion will allow you to reach the higher limits of AI intelligence and become a competitor in the market.\n\nDr. Kai-Fu Lee, Chairman & CEO ofSinovation Venturesand01.AI, says that your passion is what’s going to help you choose your next step. If you want to be the best programmer out there and that is what drives you, then learn how to code and develop your career based on these technical skills. However, if you are going into a programmer career based on the salary, then you’re better off forgetting how to code, learning everything you can about AI, and building something based on AI and your passion.\n\n#Wrapping Up\n\nThese are three short and sweet lessons that are very important for the younger generation or for anybody who is looking to get into the AI industry. Remember, you don’t have to start from the foundations of learning how to code to be part of the sector, but there is also no harm in doing so.\n\nNisha Aryais a data scientist, freelance technical writer, and an editor and community manager for KDnuggets. She is particularly interested in providing data science career advice or tutorials and theory-based knowledge around data science. Nisha covers a wide range of topics and wishes to explore the different ways artificial intelligence can benefit the longevity of human life. A keen learner, Nisha seeks to broaden her tech knowledge and writing skills, while helping guide others.\n\nMore On This Topic\n\n6 Problems of LLMs That LangChain is Trying to AssessHow Hard is it to Get into FAANG CompaniesWhat Junior ML Engineers Actually Need to Know to Get Hired?We Don't Need Data Scientists, We Need Data EngineersShort and Fun Courses to Get You Up to Speed About Generative AI5 Reasons Why You Should Get Certified\n\n"
    },
    {
        "title": "5 Lesser-Known Python Features Every Data Scientist Should Know",
        "link": "https://www.kdnuggets.com/5-lesser-known-python-features-every-data-scientist-should-know",
        "date": "2025-08-18T00:00:00+00:00",
        "content": "Image by Editor | ChatGPT\n\n[Image: 5 Lesser-Known Python Features Every Data Scientist Should Know] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: 5 Lesser-Known Python Features Every Data Scientist Should Know] https://www.kdnuggets.com/wp-content/uploads/kdn-gulati-5-lesser-known-python-features.png\n\n#Introduction\n\nPythonis one of the most popular languages used in the data science sphere, valued for its simplicity, versatility, and powerful ecosystem of libraries, includingNumPy,pandas,scikit-learn, andTensorFlow. While these tools provide much of the heavy lifting, Python itself includes a range of features that can help you write cleaner, faster, and more efficient code. Many of these capabilities go unnoticed, yet they can improve how you structure and manage your projects.\n\nIn this article, we explore five lesser-known but beneficial Python features that every data scientist should have in their toolkit.\n\n#1. TheelseClause on Loops\n\nDid you knowforandwhileloops in Python can have anelseclause?\n\nWhile this may sound counterintuitive at first, theelseblock executes only when the loop completes without abreakstatement. This is useful when you search through a dataset and want to run some logic only if a specific condition was never met.\n\nfor row in dataset:\r\n    if row['target'] == 'desired_value':\r\n        print(\"Found!\")\r\n        break\r\nelse:\r\n    print(\"Not found.\")\n\nIn this snippet, theelseblock executes only when the loop finishes without encountering a break. This lets you avoid creating extra flags or conditions outside the loop.\n\n#2. ThedataclassesModule\n\nThedataclassesmodule, introduced in Python 3.7, provides a decorator and helper functions that automatically generate special methods like__init__(),__repr__(), and__eq__()for your classes. This is useful in data science when you need lightweight classes to store parameters, results, or configuration settings without writing repetitive boilerplate code.\n\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass ExperimentConfig:\r\n    learning_rate: float\r\n    batch_size: int\r\n    epochs: int\n\nWith@dataclass, you get a clean constructor, a readable string representation, and comparison capabilities.\n\n#3. The Walrus Operator (:=)\n\nThewalrus operator(:=), introduced in Python 3.8, lets you assign values to variables as part of an expression. This is useful when you want to both calculate and test a value without repeating the calculation in multiple places.\n\ndata = [1, 2, 3, 4, 5]\r\n\r\nif (avg := sum(data) / len(data)) > 3:\r\n    print(f\"Average is {avg}\")\n\nHere,avgis assigned and checked at the same time. This removes the need for another line and makes your code easier to read.\n\n#4.enumerate()for Indexed Loops\n\nWhen you need both the index and the value while iterating,enumerate()is the most Pythonic way to do it. It takes any iterable (like a list, tuple, or string) and returns pairs of (index, value) as you loop.\n\nfor i, row in enumerate(data):\r\n    print(f\"Row {i}: {row}\")\n\nThis improves readability, reduces the chance of errors, and makes your intent clearer. It's useful in data science when iterating over rows of data or results with positions that matter.\n\n#5. ThecollectionsModule\n\nPython’scollectionsmodule provides specialized container datatypes that can be more efficient and expressive than using only lists or dictionaries. Among the most popular isCounter, which can count elements in an iterable with minimal code.\n\nfrom collections import Counter\r\n\r\nword_counts = Counter(words)\r\nmost_common = word_counts.most_common(5)\n\nNeed an ordered dictionary? UseOrderedDict. Need a dictionary with default values? Trydefaultdict. These tools eliminate the need for verbose manual logic and can even improve performance in large-scale data processing.\n\n#Conclusion\n\nTools like theelseclause on loops,dataclasses, and the walrus operator can eliminate unnecessary boilerplate and make logic more concise. Functions likeenumerate()and modules likecollectionshelp you iterate, count, and organize data with elegance and efficiency. By incorporating these lesser-known gems into your workflow, you can reduce complexity, avoid common pitfalls, and focus more on solving the actual data problem rather than wrangling your code.\n\nJayita Gulatiis a machine learning enthusiast and technical writer driven by her passion for building machine learning models. She holds a Master's degree in Computer Science from the University of Liverpool.\n\nMore On This Topic\n\n10 Python Libraries Every Data Scientist Should Know12 Docker Commands Every Data Scientist Should Know10 Essential Pandas Functions Every Data Scientist Should KnowTools Every Data Scientist Should Know: A Practical Guide7 Python Libraries Every Data Engineer Should Know10 Built-In Python Modules Every Data Engineer Should Know\n\n"
    },
    {
        "title": "Crypto AI agents: How AI is revolutionizing cryptocurrencies?",
        "link": "https://datascientest.com/en/all-bout-crypto-ai-agents",
        "date": "2025-08-18T00:00:00+00:00",
        "content": "On one hand, we have the ultra-dynamic field of cryptocurrencies. Many believe it is teeming with opportunities just waiting to be grasped. On the other hand, there’s artificial intelligence, offering its analytical prowess at any time of the day or night. By combining these two forces, can we achieve the best of both worlds?\nInvesting in\ncryptocurrency\ncan become a nightmare: monitoring market fluctuations, analyzing extensive volumes of data, and managing the inherent stress of\ntrading\nis quite a challenge. Often, an opportunity might arise at 3 AM in your local time. How can one ensure they don’t miss out on major opportunities?\nRecently, a burgeoning technology promises to revolutionize this practice:\ncrypto AI agents\n.\nWhat is a crypto AI?\nThe operation of a crypto AI agent revolves around three steps:\nData collection\nThe agent gathers information from various sources. For instance, it analyzes data from\ntrading\nplatforms such as\nBinance\nor\nCoinbase\n, along with social networks like Telegram, Discord, YouTube, or Reddit, as well as specialized news sites. New models allow agents to perceive the emotional tone of tweets or news articles. They may utilize generative AI (like\nGPT-4o\nor\nClaude 3\n) for this purpose.\nIntelligent analysis\nThe agent uses AI to best interpret this data, spot opportunities, anticipate market movements, and determine optimal strategies.\nAutomated execution\nBased on a comprehensive analysis of large amounts of data and the best synthesis, the agent autonomously executes transactions it considers most optimal.\nThese features are complemented by continuous learning capabilities, enabling the agents to consistently refine their performance.\nTaking crypto AI agents further\nThe first stars of AI-optimized trading\nSeveral crypto AI agents have already gained recognition:\nAIXBT\nAIXBT quickly became popular due to its\naccurate crypto analyses\npublished on X (Twitter). It has shown the capability to identify promising trends early on. It took only a few months to amass nearly 500,000 subscribers.\nGriffain\nThis agent specializes in the swift analysis of thousands of cryptocurrencies on the\nSolana\nblockchain\n, as well as less prominent blockchains like Aptos and Sui, whose technical features make them strong candidates for AI agents.\nNumerai\nThis approach is unique: Numerai combines the predictions of thousands of\ndata scientists\nto deliver profound financial analyses.\nAnd others…\nWe can also mention\nArkham Intelligence\n, which employs AI to track the flow of funds from large investors and was acquired by Binance in 2024.\nTensorTrade\nallows the creation of customized agents with just a few clicks.\nKnow more about AI-enhanced trading\nThe rise of crypto AI agent frameworks\nTo aid in the development of crypto AI agents, several robust frameworks have emerged:\nElizaOS\nThis is an intelligent OS that allows for the creation of highly advanced agents thanks to a large developer community. It now incorporates\nopen-source AI\nmodels (\nLlama 3\nfrom Meta,\nMistral 2\n).\nVirtuals Protocol\nVirtuals Protocol was designed to make the creation of crypto AI agents accessible to all — without requiring advanced technical skills. It facilitates the “tokenization” of agents, creating a new market where anyone can become a stakeholder in a high-performing agent. In March 2025, Virtuals Protocol raised 50 million dollars, reflecting growing investor confidence.\nFetch.ai\nThis system aims to replace traditional smart contracts (applications that manage cryptocurrencies) with AI agents capable of complex economic operations.\nFetch.ai\nhas merged with two other major players; SingularityNET and Ocean Protocol to form the\nSuperintelligence Alliance\n(ASI).\nRitual\nToday, nearly all generative AIs operate on servers hosted by giants like\nAmazon, Google, or Microsoft\n. Ritual is a burgeoning project that aims to enable the execution of AI models without relying on such centralized cloud services.\nMastering the use of crypto AI agents\nTowards collective intelligences\nIn the particularly dynamic world of cryptocurrencies, the volume of information to process often exceeds the capacity of a single\nAI agent\n, no matter how sophisticated it is.\nHowever, the technology of crypto AI agents is rapidly evolving. Some recent models employ a form of collective intelligence — as is the case with Fetch.ai mentioned above. This is referred to as\nmulti-agent (MAA)\n.\nIn simple terms, there are multiple autonomous agents working together to solve complex problems. Each agent has its own capabilities concerning\nanalysis\n,\ndecision-making\n, and\nlearning\n. The uniqueness of MAA lies in their interaction, sharing of information, and collaboration towards a common goal.\nThe best of worlds? Not so fast…\nIn the year 2024 alone, the AI market dedicated to crypto experienced a 320% growth in terms of active users, according to a report by Messari. McKinsey estimates that\n60% of crypto trades will be managed by AI by 2026\n. Gartner predicts that AI agents will dominate\ndecentralized crypto finance (DeFi)\nby 2027.\nHowever, despite their many strengths,\ncrypto AI agents\nare not without risks. Algorithmic errors can lead to significant losses. For example, in February 2025, a misconfigured AI agent resulted in a 12 million dollar loss on Solana. Moreover, “model poisoning” attacks (manipulation of training data) are on the rise, according to Halborn’s 2025 report.\nThus, prudence should remain paramount: it is wise to diversify investments, not overly rely on such tools, and remain vigilant about their operation.\nTraining in cryptocurrency trading"
    },
    {
        "title": "Catch up on the newest features in August’s Gemini Drop.",
        "link": "https://blog.google/products/gemini/gemini-drop-august-2025/",
        "date": "2025-08-15T16:00:00+00:00",
        "content": "Catch up on the newest features in August’s Gemini Drop.\n\nGemini Dropsare our regular update on what’s new in the Gemini app. Here's a look at the latest tools and features:You can now useStorybookto turn cherished memories, inside jokes and complex concepts into an illustrated storybook.For Ultra subscribers,Deep Thinkis an improved reasoning mode that helps you tackle highly complex problems in math and coding.You can now useTemporary Chatsto have conversations that won’t be saved to your history or influence future responses.Gemini can use your past chats to give you more personalized and relevantanswers.Just in time for the new school year: you can now study smarter by creating practice quizzes and flash cards from your notes, go deeper on complex topics withGuided Learningand get help organizing study guides, fine-tuning your writing and more. And don’t forget to check out theFree Pro Plan for Students.\n\nGemini Dropsare our regular update on what’s new in the Gemini app. Here's a look at the latest tools and features:\n\nRelated stories\n\nGemini adds Temporary Chats and new personalization features\n\nToday, we are updating the Gemini app so that it learns about your preferences the more you use it.\n\nNew Gemini app tools to help students learn, understand and study even better\n\nGuided Learning in Gemini: From answers to understanding\n\nBringing the best of AI to college students for free"
    },
    {
        "title": "Stress Testing FastAPI Application",
        "link": "https://www.kdnuggets.com/stress-testing-fastapi-application",
        "date": "2025-08-15T00:00:00+00:00",
        "content": "Image by Author\n\n[Image: Stress Testing FastAPI Application] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Stress Testing FastAPI Application] https://www.kdnuggets.com/wp-content/uploads/awan_stress_testing_fastapi_application_5.png\n\n#Introduction\n\nStress testing is crucial for understanding how your application behaves under heavy load. For machine learning-powered APIs, it is especially important because model inference can be CPU-intensive. By simulating a large number of users, we can identify performance bottlenecks, determine the capacity of our system, and ensure reliability.\n\nIn this tutorial, we will be using:\n\nFastAPI:A modern, fast (high-performance) web framework for building APIs with Python.Uvicorn:An ASGI server to run our FastAPI application.Locust:An open-source load testing tool. You define user behavior with Python code, and swarm your system with hundreds of simultaneous users.Scikit-learn:For our example machine learning model.\n\n#1. Project Setup and Dependencies\n\nSet up the project structure and install the necessary dependencies.\n\nCreaterequirements.txtfile and add the following Python packages:fastapi==0.115.12\r\nlocust==2.37.10\r\nnumpy==2.3.0\r\npandas==2.3.0\r\npydantic==2.11.5\r\nscikit-learn==1.7.0\r\nuvicorn==0.34.3\r\norjson==3.10.18Open your terminal, create a virtual environment, and activate it.python -m venv venv\r\nvenv\\Scripts\\activateInstall all the Python packages using therequirements.txtfile.pip install -r requirements.txt\n\nfastapi==0.115.12\r\nlocust==2.37.10\r\nnumpy==2.3.0\r\npandas==2.3.0\r\npydantic==2.11.5\r\nscikit-learn==1.7.0\r\nuvicorn==0.34.3\r\norjson==3.10.18\n\npython -m venv venv\r\nvenv\\Scripts\\activate\n\npip install -r requirements.txt\n\n#2. Building the FastAPI Application\n\nIn this section, we will create a file for training the Regression model, for pydantic models, and the FastAPI application.\n\nThisml_model.pyhandles the machine learning model. It uses a singleton pattern to ensure only one instance of the model is loaded. The model is a Random Forest Regressor trained on the California housing dataset. If a pre-trained model (model.pkl and scaler.pkl) doesn't exist, it trains and saves a new one.\n\napp/ml_model.py:\n\nimport os\r\nimport threading\r\n\r\nimport joblib\r\nimport numpy as np\r\nfrom sklearn.datasets import fetch_california_housing\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nclass MLModel:\r\n    _instance = None\r\n    _lock = threading.Lock()\r\n\r\n    def __new__(cls):\r\n        if cls._instance is None:\r\n            with cls._lock:\r\n                if cls._instance is None:\r\n                    cls._instance = super().__new__(cls)\r\n        return cls._instance\r\n\r\n    def __init__(self):\r\n        if not hasattr(self, \"initialized\"):\r\n            self.model = None\r\n            self.scaler = None\r\n            self.model_path = \"model.pkl\"\r\n            self.scaler_path = \"scaler.pkl\"\r\n            self.feature_names = None\r\n            self.initialized = True\r\n            self.load_or_create_model()\r\n\r\n    def load_or_create_model(self):\r\n        \"\"\"Load existing model or create a new one using California housing dataset\"\"\"\r\n        if os.path.exists(self.model_path) and os.path.exists(self.scaler_path):\r\n            self.model = joblib.load(self.model_path)\r\n            self.scaler = joblib.load(self.scaler_path)\r\n            housing = fetch_california_housing()\r\n            self.feature_names = housing.feature_names\r\n            print(\"Model loaded successfully\")\r\n        else:\r\n            print(\"Creating new model...\")\r\n            housing = fetch_california_housing()\r\n            X, y = housing.data, housing.target\r\n            self.feature_names = housing.feature_names\r\n\r\n            X_train, X_test, y_train, y_test = train_test_split(\r\n                X, y, test_size=0.2, random_state=42\r\n            )\r\n\r\n            self.scaler = StandardScaler()\r\n            X_train_scaled = self.scaler.fit_transform(X_train)\r\n\r\n            self.model = RandomForestRegressor(\r\n                n_estimators=50,  # Reduced for faster predictions\r\n                max_depth=8,  # Reduced for faster predictions\r\n                random_state=42,\r\n                n_jobs=1,  # Single thread for consistency\r\n            )\r\n            self.model.fit(X_train_scaled, y_train)\r\n\r\n            joblib.dump(self.model, self.model_path)\r\n            joblib.dump(self.scaler, self.scaler_path)\r\n\r\n            X_test_scaled = self.scaler.transform(X_test)\r\n            score = self.model.score(X_test_scaled, y_test)\r\n            print(f\"Model R² score: {score:.4f}\")\r\n\r\n    def predict(self, features):\r\n        \"\"\"Make prediction for house price\"\"\"\r\n        features_array = np.array(features).reshape(1, -1)\r\n        features_scaled = self.scaler.transform(features_array)\r\n        prediction = self.model.predict(features_scaled)[0]\r\n        return prediction * 100000\r\n\r\n    def get_feature_info(self):\r\n        \"\"\"Get information about the features\"\"\"\r\n        return {\r\n            \"feature_names\": list(self.feature_names),\r\n            \"num_features\": len(self.feature_names),\r\n            \"description\": \"California housing dataset features\",\r\n        }\r\n\r\n# Initialize model as singleton\r\nml_model = MLModel()\n\nThepydantic_models.pyfile defines the Pydantic models for request and response data validation and serialization.\n\napp/pydantic_models.py:\n\nfrom typing import List\r\n\r\nfrom pydantic import BaseModel, Field\r\n\r\nclass PredictionRequest(BaseModel):\r\n    features: List[float] = Field(\r\n        ...,\r\n        description=\"List of 8 features: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\",\r\n        min_length=8,\r\n        max_length=8,\r\n    )\r\n\r\n    model_config = {\r\n        \"json_schema_extra\": {\r\n            \"examples\": [\r\n                {\"features\": [8.3252, 41.0, 6.984, 1.024, 322.0, 2.556, 37.88, -122.23]}\r\n            ]\r\n        }\r\n    }\n\napp/main.py:This file is the core FastAPI application, defining the API endpoints.\n\nimport asyncio\r\nfrom contextlib import asynccontextmanager\r\n\r\nfrom fastapi import FastAPI, HTTPException\r\nfrom fastapi.responses import ORJSONResponse\r\n\r\nfrom .ml_model import ml_model\r\nfrom .pydantic_models import (\r\n    PredictionRequest,\r\n)\r\n\r\n@asynccontextmanager\r\nasync def lifespan(app: FastAPI):\r\n    # Pre-load the model\r\n    _ = ml_model.get_feature_info()\r\n    yield\r\n\r\napp = FastAPI(\r\n    title=\"California Housing Price Prediction API\",\r\n    version=\"1.0.0\",\r\n    description=\"API for predicting California housing prices using Random Forest model\",\r\n    lifespan=lifespan,\r\n    default_response_class=ORJSONResponse,\r\n)\r\n\r\n@app.get(\"/health\")\r\nasync def health_check():\r\n    \"\"\"Health check endpoint\"\"\"\r\n    return {\"status\": \"healthy\", \"message\": \"Service is operational\"}\r\n\r\n@app.get(\"/model-info\")\r\nasync def model_info():\r\n    \"\"\"Get information about the ML model\"\"\"\r\n    try:\r\n        feature_info = await asyncio.to_thread(ml_model.get_feature_info)\r\n        return {\r\n            \"model_type\": \"Random Forest Regressor\",\r\n            \"dataset\": \"California Housing Dataset\",\r\n            \"features\": feature_info,\r\n        }\r\n    except Exception:\r\n        raise HTTPException(\r\n            status_code=500, detail=\"Error retrieving model information\"\r\n        )\r\n\r\n@app.post(\"/predict\")\r\nasync def predict(request: PredictionRequest):\r\n    \"\"\"Make house price prediction\"\"\"\r\n    if len(request.features) != 8:\r\n        raise HTTPException(\r\n            status_code=400,\r\n            detail=f\"Expected 8 features, got {len(request.features)}\",\r\n        )\r\n    try:\r\n        prediction = ml_model.predict(request.features)\r\n        return {\r\n            \"prediction\": float(prediction),\r\n            \"status\": \"success\",\r\n            \"features_used\": request.features,\r\n        }\r\n    except ValueError as e:\r\n        raise HTTPException(status_code=400, detail=str(e))\r\n    except Exception:\r\n        raise HTTPException(status_code=500, detail=\"Prediction error\")\n\nKey points:\n\nlifespanmanager: Ensures the ML model is loaded during application startup.asyncio.to_thread: This is crucial because scikit-learn's predict method is CPU-bound (synchronous). Running it in a separate thread prevents it from blocking FastAPI's asynchronous event loop, allowing the server to handle other requests concurrently.\n\nEndpoints:\n\n/health: A simple health check./model-info: Provides metadata about the ML model./predict: Accepts a list of features and returns a house price prediction.\n\nrun_server.py:It contains the script that is used to run the FastAPI application using Uvicorn.\n\nimport uvicorn\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    uvicorn.run(\"app.main:app\", host=\"localhost\", port=8000, workers=4)\n\nAll the files and configurations are available at the GitHub repository:kingabzpro/Stress-Testing-FastAPI\n\n#3. Writing the Locust Stress Test\n\nNow, let's create the stress test script using Locust.\n\ntests/locustfile.py:This file defines the behavior of simulated users.\n\nimport json\r\nimport logging\r\nimport random\r\n\r\nfrom locust import HttpUser, task\r\n\r\n# Reduce logging to improve performance\r\nlogging.getLogger(\"urllib3\").setLevel(logging.WARNING)\r\n\r\nclass HousingAPIUser(HttpUser):\r\n    def generate_random_features(self):\r\n        \"\"\"Generate random but realistic California housing features\"\"\"\r\n        return [\r\n            round(random.uniform(0.5, 15.0), 4),  # MedInc\r\n            round(random.uniform(1.0, 52.0), 1),  # HouseAge\r\n            round(random.uniform(2.0, 10.0), 2),  # AveRooms\r\n            round(random.uniform(0.5, 2.0), 2),  # AveBedrms\r\n            round(random.uniform(3.0, 35000.0), 0),  # Population\r\n            round(random.uniform(1.0, 10.0), 2),  # AveOccup\r\n            round(random.uniform(32.0, 42.0), 2),  # Latitude\r\n            round(random.uniform(-124.0, -114.0), 2),  # Longitude\r\n        ]\r\n\r\n    @task(1)\r\n    def model_info(self):\r\n        \"\"\"Test health endpoint\"\"\"\r\n        with self.client.get(\"/model-info\", catch_response=True) as response:\r\n            if response.status_code == 200:\r\n                response.success()\r\n            else:\r\n                response.failure(f\"Model info failed: {response.status_code}\")\r\n\r\n    @task(3)\r\n    def single_prediction(self):\r\n        \"\"\"Test single prediction endpoint\"\"\"\r\n        features = self.generate_random_features()\r\n\r\n\r\n        with self.client.post(\r\n            \"/predict\", json={\"features\": features}, catch_response=True, timeout=10\r\n        ) as response:\r\n            if response.status_code == 200:\r\n                try:\r\n                    data = response.json()\r\n                    if \"prediction\" in data:\r\n                        response.success()\r\n                    else:\r\n                        response.failure(\"Invalid response format\")\r\n                except json.JSONDecodeError:\r\n                    response.failure(\"Failed to parse JSON\")\r\n            elif response.status_code == 503:\r\n                response.failure(\"Service unavailable\")\r\n            else:\r\n                response.failure(f\"Status code: {response.status_code}\")\n\nKey points:\n\nEach simulated user will wait between 0.5 and 2 seconds between executing tasks.Creates realistic random feature data for the prediction requests.Each user will make one health_check request and 3 single_prediction requests.\n\n#4. Running the Stress Test\n\nTo evaluate the performance of your application under load, begin by starting your asynchronous machine learning application in one terminal.python run_server.pyModel loaded successfully\r\nINFO:     Started server process [26216]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)Open your browser and navigate to http://localhost:8000/docs. Use the interactive API documentation to test your endpoints and ensure they are functioning correctly.Open a new terminal window, activate the virtual environment, and navigate to your project's root directory to run Locust with the Web UI:locust -f tests/locustfile.py --host http://localhost:8000Access the Locust web UI athttp://localhost:8089in your browser.In the Locust web UI, set the total number of users to 500, the spawn rate to 10 users per second, and run it for a minute.During the test, Locust will display real-time statistics, including the number of requests, failures, and response times for each endpoint.Once the test is complete, click on the Charts tab to view interactive graphs showing the number of users, requests per second, and response times.To run Locust without the web UI and automatically generate an HTML report, use the following command:locust -f tests/locustfile.py --host http://localhost:8000 --users 500 --spawn-rate 10 --run-time 60s --headless  --html report.html\n\npython run_server.py\n\nModel loaded successfully\r\nINFO:     Started server process [26216]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n\n[Image: Stress Testing FastAPI Application] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Stress Testing FastAPI Application] https://www.kdnuggets.com/wp-content/uploads/awan_stress_testing_fastapi_application_4.png\n\nlocust -f tests/locustfile.py --host http://localhost:8000\n\nAccess the Locust web UI athttp://localhost:8089in your browser.\n\n[Image: Stress Testing FastAPI Application] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Stress Testing FastAPI Application] https://www.kdnuggets.com/wp-content/uploads/awan_stress_testing_fastapi_application_3.png\n\n[Image: Stress Testing FastAPI Application] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Stress Testing FastAPI Application] https://www.kdnuggets.com/wp-content/uploads/awan_stress_testing_fastapi_application_2.png\n\n[Image: Stress Testing FastAPI Application] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Stress Testing FastAPI Application] https://www.kdnuggets.com/wp-content/uploads/awan_stress_testing_fastapi_application_1.png\n\nlocust -f tests/locustfile.py --host http://localhost:8000 --users 500 --spawn-rate 10 --run-time 60s --headless  --html report.html\n\nAfter the test finishes, an HTML report named report.html will be saved in your project directory for later review.\n\n[Image: Stress Testing FastAPI Application] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Stress Testing FastAPI Application] https://www.kdnuggets.com/wp-content/uploads/awan_stress_testing_fastapi_application_6.png\n\n#Final Thoughts\n\nOur app can handle a large number of users as we are using a simple machine learning model. The results show that the model-info endpoint has a greater response time than the prediction, which is impressive. This is the best-case scenario for testing your application locally before pushing it to production.\n\nIf you would like to experience this setup firsthand, please visit thekingabzpro/Stress-Testing-FastAPIrepository and follow the instructions in the documentation.\n\nAbid Ali Awan(@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness.\n\nMore On This Topic\n\nHypothesis Testing and A/B TestingCreating a Web Application to Extract Topics from Audio with PythonRAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?Build An AI Application with Python in 10 Easy StepsAnythingLLM: The LLM Application You've Been Waiting ForBuilding AI Application with Gemini 2.0\n\n"
    },
    {
        "title": "How I Use AI Agents as a Data Scientist in 2025",
        "link": "https://www.kdnuggets.com/how-i-use-ai-agents-as-a-data-scientist-in-2025",
        "date": "2025-08-15T00:00:00+00:00",
        "content": "Image by Author\n\n[Image: How I Use AI Agents as a Data Scientist in 2025] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: How I Use AI Agents as a Data Scientist in 2025] https://www.kdnuggets.com/wp-content/uploads/How-I-Use-AI-Agents-as-a-Data-Scientist.png\n\n#Introduction\n\nAs data scientists, we wear so many hats on the job that it often feels like multiple careers rolled into one. In a single workday, I have to:\n\nBuild data pipelines withSQLandPythonUse statistics to analyze dataCommunicate recommendations to stakeholdersConsistently monitor product performance and generate reportsRun experiments to help the company decide whether to launch a product\n\nAnd this is just half of it.\n\nBeing a data scientist is exciting because it's one of the most versatile fields in tech: you get exposure to so many different aspects of the business and can visualize the impact of products on everyday users.\n\nBut the downside? It feels like you are always playing catch-up.\n\nIf a product launch performs poorly, you need to figure out why — and you must do so instantly. In the meantime, if a stakeholder wants to understand the impact of launching feature A instead of feature B, you need to design an experiment quickly and explain the results to them in a way that’s easy to understand.\n\nYou can’t be too technical in your explanation, but you also can’t be too vague. You must find a middle ground that balances interpretability with analytical rigor.\n\nBy the end of a workday,it sometimes feels like I've just run a marathon. Only to wake up and do it all again the next day. So when I get the opportunity to automate parts of my job with AI, I take it.\n\nRecently, I have started incorporating AI agents into my data science workflows.\n\nThis has made me more efficient at my job, and I can answer business questions with data much faster than I used to.\n\nIn this article, I will explain exactly how I use AI agents to automate parts of my data science workflow. Specifically, we will explore:\n\nHow I typically perform a data science workflow without AIThe steps taken to automate the workflow with AIThe exact tools I use and how much time this has saved me\n\nBut before we get into that, let’s revisit what exactly an AI agent is and why there is so much hype around them.\n\n#What Are AI Agents?\n\nAI agents are large language model (LLM)-powered systems that can perform tasks automatically by planning and reasoning through a problem. They can be used to automate advanced workflows without explicit direction from the user.\n\nThis can look like running a single command and having an LLM execute an end-to-end workflow while making decisions and adapting its approach throughout the process. You can use this time to focus on other tasks without needing to intervene or monitor each step.\n\n#How I Use AI Agents to Automate Experimentation in Data Science\n\nExperimentation is a huge part of a data science job.\n\nCompanies like Spotify, Google, and Meta always experiment before they release a new product to understand:\n\nWhether the new product will provide a high return on investment and is worth the resources allocated to building itIf the product will have a long-term positive impact on the platformUser sentiment around this product launch\n\nData scientists typically perform A/B tests to determine the effectiveness of a new feature or product launch. To learn more about A/B testing in data science, you can readthis guide on A/B testing.\n\nCompanies can run up to 100 experiments a week. Experiment design and analysis can be a highly repetitive process, which is why I decided to try to automate it using AI agents.\n\nHere’s how I typically analyze the results of an experiment, a process that takes around three days to a week:\n\nBuild SQL pipelines to extract the A/B test data that flows in from the systemQuery these pipelines and perform exploratory data analysis (EDA) to determine the type of statistical test to useWrite Python code to run statistical tests and visualize this dataGenerate a recommendation (for example, roll out this feature to 100% of our users)Present this data in the form of an Excel sheet, document, or a slide deck and explain the results to stakeholders\n\nSteps 2 and 3 are the most time-consuming because experiment results aren’t always straightforward.\n\nFor example, when deciding whether to roll out a video ad or an image ad, we may get contradictory results. An image ad might generate more immediate purchases, leading to higher short-term revenue. However, video ads might lead to better user retention and loyalty, which means that customers make more repeat purchases. This leads to higher long-term revenue.\n\nIn this case, we need to gather more supporting data points to make a decision on whether to launch image or video ads. We might have to use different statistical techniques and perform some simulations to see which approach aligns best with our business goals.\n\nWhen this process is automated with an AI agent, it removes a lot of manual intervention. We can have AI gather data and perform this deep-dive analysis for us, which removes the analytical heavy lifting that we typically do.\n\nHere’s what the automated A/B test analysis with an AI agent looks like:\n\nI useCursor, an AI editor that can access your codebase and automatically write and edit your code.Using the Model Context Protocol (MCP), Cursor gains access to the data lake where raw experiment data flows intoCursor then automatically builds a pipeline to process experiment data, and accesses the data lake again to join this with other relevant data tablesAfter creating all the necessary pipelines, it performs EDA on these tables and automatically determines the best statistical technique to use to analyze the results of the A/B testIt runs the chosen statistical test and analyzes the output, automatically creating a comprehensive HTML report of the output in a format that is presentable to business stakeholders\n\nThe above is an end-to-end experiment automation framework with an AI agent.\n\nOf course, once this process is completed, I review the results of the analysis and go through the steps taken by the AI agent. I have to admit that this workflow isn’t always seamless. AI does hallucinate and needs a ton of prompting and examples of prior analyses before it can come up with its own workflow. The \"garbage in, garbage out\" principle definitely applies here, and I spent almost a week curating examples and building prompt files to ensure that Cursor had all the relevant information needed to run this analysis.\n\nThere was a lot of back and forth and multiple iterations before the automated framework performed as expected.\n\nNow that this AI agent works, however, I am able to dramatically reduce the amount of time spent on analyzing the results of A/B tests. While the AI agent performs this workflow, I can focus on other tasks.\n\nThis takes tasks off my plate, making me a slightly less busy data scientist. I also get to present results to stakeholders quickly, and the shorter turnaround time helps the entire product team make quicker decisions.\n\n#Why You Must Learn AI Agents for Data Science\n\nEvery data professional I know has incorporated AI into their workflow in some way. There's a top-down push for this in organizations to make quicker business decisions, launch products faster, and stay ahead of the competition. I believe that AI adoption is crucial for data scientists to stay relevant and remain competitive in this job market.\n\nAnd in my experience, creating agentic workflows to automate parts of our jobs requires us to upskill. I’ve had to learn new tools and techniques like MCP configuration, AI agent prompting (which is different from typing a prompt intoChatGPT), and workflow orchestration. The initial learning curve is worth it because it saves hours once you're able to automate parts of your job.\n\nIf you are a data scientist or an aspiring one, I recommend learning how to build AI-assisted workflows early in your career. This is quickly becoming an industry expectation rather than just a nice-to-have, and you should start positioning yourself for the near future of data roles.\n\nTo get started, you canwatch this videofor a step-by-step guide on how to learn agentic AI for free.\n\nNatassha Selvarajis a self-taught data scientist with a passion for writing. Natassha writes on everything data science-related, a true master of all data topics. You can connect with her onLinkedInor check out herYouTube channel.\n\nMore On This Topic\n\nWhat Data Scientists Need to Know About AI Agents and Autonomous SystemsAgentGPT: Autonomous AI Agents in your BrowserWhy You Need To Know About Autonomous AI AgentsBringing Human and AI Agents Together for Enhanced Customer ExperienceThe Growth Behind LLM-based Autonomous AgentsThe Easiest Way to Create Real-Time AI Voice Agents\n\n"
    },
    {
        "title": "The Future of LLM Development is Open Source",
        "link": "https://www.kdnuggets.com/the-future-of-llm-development-is-open-source",
        "date": "2025-08-14T00:00:00+00:00",
        "content": "Image by Editor | ChatGPT\n\n[Image: The Future of LLM Development is Open Source] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: The Future of LLM Development is Open Source] https://www.kdnuggets.com/wp-content/uploads/kdn-future-of-llm-development-open-source.png\n\n#Introduction\n\nThe future of large language models (LLMs) won’t be dictated by a handful of corporate labs. It will be shaped by thousands of minds across the globe, iterating in the open, pushing boundaries without waiting for boardroom approval. Theopen-source movement has already shown it can keep pace with, and in some areas even outmatch, its proprietary counterparts.Deepseek, anyone?\n\nWhat started as a trickle of leaked weights and hobbyist builds is now a roaring current: organizations likeHugging Face,Mistral, andEleutherAIare proving that decentralization doesn’t mean disorder — it means acceleration. We’re entering a phase where openness equals power. The walls are coming down. And those who insist on closed gates may find themselves defending castles that might crumble easily.\n\n#Open Source LLMs Aren’t Just Catching Up, They’re Winning\n\nLook past the marketing gloss of trillion-dollar companies and you’ll see a different story unfolding. LLaMA 2, Mistral 7B,and Mixtral are outperforming expectations, punching above their weight against closed models that require magnitudes more parameters and compute. Open-source innovation is no longer reactionary — it’s proactive.\n\nThe reasons are structural, in particularbecause proprietary LLMs are hamstrung by corporate risk management, legal red tape, and a culture of perfectionism. Open-source projects? They ship. They iterate fast, they break things, and they rebuild better. They can crowdsource both experimentation and validation in ways no in-house team could replicate at scale. A single Reddit thread can surface bugs, uncover clever prompts, and expose vulnerabilities within hours of a release.\n\nAdd to that the emerging ecosystem of contributors — devs fine-tuning models on personal data, researchers building evaluation suites, engineers crafting inference runtimes — and what you get is a living, breathing engine of advancement. In a way,closed AI will always be reactive. open AI is alive.\n\n#Decentralization Doesn’t Mean Chaos — It Means Control\n\nCritics love to frame open-source LLM development as the Wild West, brimming with risks of misuse. What they ignore is that openness doesn’t negate accountability — it enables it. Transparency fosters scrutiny. Forks introduce specialization. Guardrails can be openly tested, debated, and improved. The community becomes both innovator and watchdog.\n\nContrast that with the opaque model releases from closed companies, where bias audits are internal, safety methods are secret, and critical details are redacted under “responsible AI” pretexts. The open-source world may be messier,but it’s also significantly more democratic and accessible. It acknowledges that power over language — and therefore thought — shouldn’t be consolidated in the hands of a few Silicon Valley CEOs.\n\nOpen LLMs can also empower organizations that otherwise would have been locked out — startups, researchers in low-resource countries, educators, and artists. With the right model weights and some creativity, you can now build your own assistant, tutor, analyst, or co-pilot, whether it’s writing code, automating workflows, or enhancingKubernetesclusters, without licensing fees or API limits. That’s not an accident. That’s a paradigm shift.\n\n#Alignment and Safety Won’t Be Solved in Boardrooms\n\nOne of the most persistent arguments against open LLMs is safety, especially concerns around alignment, hallucination, and misuse. But here’s the hard truth: those issues plague closed models just as much, if not more. In fact, locking the code behind a firewall doesn’t prevent misuse. It prevents understanding.\n\nOpen models allow for real, decentralized experimentation in alignment techniques. Community-led red teaming,crowd-sourced RLHF (reinforcement learning from human feedback), and distributed interpretability research are already thriving. Open source invites more eyes on the problem, more diversity of perspectives, and more chances to discover techniques that actually generalize.\n\nMoreover, open development allows for tailored alignment. Not every community or language group needs the same safety preferences. A one-size-fits-all “guardian AI” from a U.S. corporation will inevitably fall short when deployed globally. Local alignmentdone transparently, with cultural nuance, requires access. And access starts with openness.\n\n#The Economic Incentive Is Shifting Too\n\nThe open-source momentum isn’t just ideological — it’s economic. The companies that lean into open LLMs are starting to outperform those who guard their models like trade secrets. Why? Because ecosystems beat monopolies. A model that others can build on quickly becomes the default. And in AI, being the default means everything.\n\nLook at what happened withPyTorch,TensorFlow,andHugging Face’s Transformers library. The most widely adopted tools in AI are those that embraced the open-source ethos early. Now we’re seeing the same trend play out with base models: developers want access, not APIs. They want modifiability, not terms of service.\n\nMoreover,the cost of developing a foundational model has dropped significantly. With open-weight checkpoints, synthetic data bootstrapping, and quantized inference pipelines, even mid-sized companies can train or fine-tune their own LLMs. The economic moat that Big AI once enjoyed is drying up — and they know it.\n\n#What Big AI Gets Wrong About the Future\n\nThe tech giants still believe that brand, compute, and capital will carry them to AI dominance. Meta might be the only exception,with its Llama 3 model still remaining open source. But the value is drifting upstream. It’s no longer about who builds the biggest model — it’s about who builds the most usable one. Flexibility, speed, and accessibility are the new battlegrounds, and open-source wins on all fronts.\n\nJust look at how quickly the open community implements language model-related innovations:FlashAttention,LoRA,QLoRA, Mixture of Experts (MoE) routing — each adopted and re-implemented within weeks or even days. Proprietary labs can barely publish papers before GitHub has a dozen forks running on a single GPU. That agility isn’t just impressive — it’s unbeatable at scale.\n\nThe proprietary approach assumes users want magic. The open approach assumes users want agency. And as developers, researchers, and enterprises mature in their LLM use cases, they’re gravitating toward models that they can understand, shape, and deploy independently. If Big AI doesn’t pivot, it won’t be because they weren’t smart enough. It’ll be because they were too arrogant to listen.\n\n#Final Thoughts\n\nThe tide has turned. Open-source LLMs aren’t a fringe experiment anymore. They’re a central force shaping the trajectory of language AI. And as the barriers to entry fall — from data pipelines to training infrastructure to deployment stacks — more voices will join the conversation, more problems will be solved in public, and more innovation will happen where everyone can see it.\n\nThis doesn’t mean we’ll abandon all closed models. But it does mean they’ll have to prove their worth in a world where open competitors exist — and often outperform. The old default of secrecy and control is crumbling. In its place is a vibrant, global network of tinkerers, researchers, engineers, and artists who believe that true intelligence should be shared.\n\nNahla Daviesis a software developer and tech writer. Before devoting her work full time to technical writing, she managed—among other intriguing things—to serve as a lead programmer at an Inc. 5,000 experiential branding organization whose clients include Samsung, Time Warner, Netflix, and Sony.\n\nMore On This Topic\n\nOpen Assistant: Explore the Possibilities of Open and Collaborative…Closed Source VS Open Source Image AnnotationFalcon LLM: The New King of Open-Source LLMsIntroducing MPT-7B: A New Open-Source LLMLLM Apocalypse Now: Revenge of the Open Source ClonesIntroducing MetaGPT's Data Interpreter: SOTA Open Source LLM-based…\n\n"
    },
    {
        "title": "All You Need is Ollama’s New App",
        "link": "https://www.kdnuggets.com/all-you-need-is-ollamas-new-app",
        "date": "2025-08-14T00:00:00+00:00",
        "content": "Image by Editor | ChatGPT\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/kdn-wijaya-ollama-new-app-all-you-need.png\n\nIn the current era of large language model (LLM) products such as ChatGPT or Gemini, we have relied on these tools to increase our productivity by helping with various tasks, such as answering questions, summarizing documents, planning activities, and more. These tools have become a part of our everyday lives.\n\nHowever, many of these products are hosted in the cloud, and we need to access them through their platform. Moreover, each platform is typically limited to its own proprietary models and will not allow the usage of other LLMs. This is why the Ollama application was developed to help users who wish to implement various LLMs in their local environment.\n\nOllama has been around for some time and has already helped many users run language models locally. The latest update has made it even more powerful.\n\nLet's explore why Ollama's new application is becoming an essential tool for many users.\n\n#Ollama’s New App\n\nAs mentioned,Ollamais an open-source tool that runs LLMs in a local environment without relying on cloud hosting. We can explorewhich models are availableon Ollama's site, download them, and run them directly using the application interface. The application acts as a local model manager that hosts various models for us to execute freely. A local application like this benefits the user by allowing them to have freedom while still preserving data privacy and reducing any latency.\n\nThe biggest development is that Ollama now runs as a standalone GUI app, as opposed to solely via command line. Gone are the days ofnecessarilyseeking out, installing, and configuring a third party UI app (or writing your own) that could turn Ollama into a more pleasurable experience. Sure you can still do all of that, but there is no need to any longer.\n\nWith the new update, the Ollama application has become more useful than ever, and we will explore each of its features individually.\n\nThe Ollama application allows users to utilize an LLM locally by downloading the model into their local environment. You can interact with a model by selecting it and providing a prompt to get a result.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_2-scaled.png\n\nOllama keeps your conversation history, allowing you to follow up on the conversation.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_3.png\n\nIf a model is not yet available locally, the new Ollama application will automatically download it before executing the prompt. This simplifies the user experience, as they no longer need to download models separately before use.\n\nAnother new feature is the ability to chat with your files. By dragging and dropping a file onto the Ollama app, we can ask questions about its contents.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_4.png\n\nThe result is shown below, where the model accesses the file and executes the prompt based on the provided document.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_5-scaled.png\n\nIf you need to process many larger documents, you can increase Ollama’s context length through the settings. However, increasing the context length will require more memory to ensure stable performance.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_6.png\n\nOllama's new features are not limited to textual documents like PDF and Word. It also offers multimodal support, provided the chosen model can process different data types. For example, we can use a model like Llama to process an image, as shown below.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_7.png\n\nThe result is shown below.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_8.png\n\nLastly, the new Ollama application can process code files to produce documentation. This feature is especially beneficial for developers.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_9.png\n\nThe result is shown below.\n\n[Image: All You Need is Ollama's New App] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: All You Need is Ollama's New App] https://www.kdnuggets.com/wp-content/uploads/All-You-Need-is-Ollamas-New-App_10.png\n\nThese are the key new features Ollama provides to improve your productivity. We encourage you to utilize them in your work.\n\n#Wrapping Up\n\nOllama is an application that allows users to run LLMs in a local environment. It is a valuable tool for any user who wants to test various models and keep their data private. Its new release offers more powerful features, including a usable chat UI, easy model downloading, the ability to chat with your files, multimodal support, and code processing.\n\nI hope this has helped!\n\nCornellius Yudha Wijayais a data science assistant manager and data writer. While working full-time at Allianz Indonesia, he loves to share Python and data tips via social media and writing media. Cornellius writes on a variety of AI and machine learning topics.\n\nMore On This Topic\n\nTextbooks Are All You Need: A Revolutionary Approach to AI TrainingAI-First Google Colab is All You NeedOllama Tutorial: Running LLMs Locally Made Super SimpleUse (Almost) Any Language Model Locally with Ollama and Hugging Face Hub5 Skills All Marketing Analytics and Data Science Pros Need TodayHuggingChat is the One LLM App You Shouldn't Overlook\n\n"
    },
    {
        "title": "AI Insights in Power BI: AI at the service of decision-making!",
        "link": "https://datascientest.com/en/all-about-ai-insights-in-power-bi",
        "date": "2025-08-14T00:00:00+00:00",
        "content": "Power BI AI Insights are the artificial intelligence features built into Microsoft’s analytics tool. These AI modules, which include predictive analytics, automated analysis, and natural language processing, can enhance your dashboards with intelligent data interpretation. Discover how to leverage them!\nMaking data speak is one thing, but with AI and the\nAI Insights integrated into Power BI\n, you can now enable data to speak for themselves! In an era where data volumes are exploding and decision-making timelines are shrinking, simply visualizing trends is no longer sufficient. One needs to\nunderstand the causes\n,\ndetect weak signals\n, and\nanticipate trends.\nArtificial intelligence in Power BI is not just a novelty; it is deeply integrated. With automated analyses, predictive models, natural language processing, summary generation, and anomaly detection, Power BI is now capable of not only showing what has happened, but also\nexplaining\nwhy,\nsuggesting\nwhat might happen, and even\nrecommending\nactions!\nWhether you’re a\ndata analyst\n,\ndata engineer\n, or business decision-maker, AI in Power BI allows you to achieve greater clarity, responsiveness, and relevance. And all this is possible\nwithout having to code a single algorithm\n. But how do you harness its full potential?\nGoing deeper into AI\nWhy Inject AI into Your Power BI Reports?\nTraditional dashboards\nhave limitations. They can describe and illustrate, but they do not explain. A\ndeclining curve\n, a\nsoaring value\n, or a\nKPI\nthat goes off track rarely lends itself to an obvious reading.\nEverything changed with the\nadvent of artificial intelligence\n. This technology adds an automated, contextualized analysis layer to\nthe data you visualize in Power BI\n. By embedding AI directly into its reports, Power BI enables the transformation of\nhistorical data into decision-making levers\n.\nRather than merely viewing numbers, you query a system that helps you interpret them. And this is done without leaving the\nPower BI environment\nor needing to train teams in Data Science. Another benefit: AI in Power BI is designed to be actionable. An insight is not merely a statistical fact; it is a\ntrigger\n.\nIt can signal an adjustment in strategy, prompt an alert, or generate a simulation scenario. In essence, it helps move closer to\nprescriptive analysis\n.\nAn Overview of Native AI Features in Power BI\nPower BI offers several ready-to-use artificial intelligence modules directly\nintegrated into the interface, requiring no code\n. The “Key Influencers” visual analysis identifies variables that most affect a targeted value.\nFor example: “What factors most influence customer subscription renewals?” The tool\nautomatically calculates correlations, ranks the variables, and presents them visually\n. Moreover, on any time series curve, Power BI can\nhighlight points identified as anomalies\n, with automated explanations.\nThis facilitates swift identification of behavioral deviations, errors, or unforeseen events. You can also\ninteract with reports using natural language\nby asking questions like “What is the revenue by region this quarter?”.\nPower BI translates the query into\nDAX\nor\nSQL\n, and generates the corresponding visualization automatically. For\nPower BI Premium\nor Fabric users, AI advances even further with the\ncreation of automated machine learning models\nvia AutoML.\nYou select a target variable (e.g., churn rate), and Power BI trains a\npredictive model in the background\n, directly from the Power BI Service interface. For more advanced projects, Power BI can connect to models deployed in Azure Machine Learning. This allows you to\nintegrate predictions or scores directly into reports\n, without inconsistencies between tools!\nLearn more about Power BI\nAI Insights in Power Query\nEven before displaying any data in a report,\nPower Query\nplays a pivotal role: this is where data is prepared, transformed, cleaned, and enriched. It is also where AI Insights take on additional significance. Through the integration of\nAzure Cognitive Services\n, Power Query (in\nPower BI Service\nor\nPower Platform Dataflows\n) offers\nready-to-use AI functions\nthat can be activated with a few clicks.\nSentiment analysis\nis ideal for extracting emotions from a\ncustomer review\nor a\nsocial media comment\n. It provides a\npositive/neutral/negative\nscore that you can use like any other column.\nLanguage detection\nis particularly useful for automatically sorting multilingual texts. With keyword extraction, Power Query isolates key concepts from text to facilitate categorization or thematic analysis.\nMoreover, with named entity recognition, the tool identifies\nnames of people\n,\norganizations\n,\nplaces\n, or\ndates\nwithin free text.\nThese processes can be automated in your data flows and executed in the cloud\n. In other words, your data arrives already “augmented” in Power BI. The end user has nothing else to do: insights are integrated into the model.\nFor technically inclined profiles, it’s possible to go further with custom functions based on\nexternal APIs\n(such as GPT models or computer vision tools), allowing for truly customized scenarios.\nBuilding an Enhanced Data Pipeline\nThe advantage of using AI Insights in Power BI is their seamless integration into the entire Power Platform ecosystem. This is no longer just about visualization. It’s a complete\ndecision-making pipeline\ncapable of capturing, analyzing, and acting.\nBy combining Power BI and\nPower Automate\n, you can\ninitiate smart actions\n. Imagine AI detects a likely increase in customer churn. With Power Automate, you can\ntrigger a scenario\n: sending automatic emails, creating a ticket in a\nCRM\n, or even alerting a sales team. The insight becomes a trigger.\nPower Apps\nallows these insights to be displayed in a\nbusiness app\n. Scores from a predictive model can be shown directly in a\nlow-code app\nbuilt with the tool. For example, a salesperson can see a customer’s churn risk, a product suggestion, or an engagement score during contact.\nUse AI insights in Power BI\nFurthermore, data enriched in Power Query can transit through\nDataverse\n, the common\ndatabase\nof the Power Platform and be cross-referenced with other sources. ML models can be hosted in\nAzure ML\nor called from\nAzure Functions\n. This allows for building an advanced analysis chain: ingestion, processing, enrichment, decision.\nThus, Dataverse and Azure form a\ncohesive AI architecture\n. It is this smooth orchestration between Power BI, Power Query, Power Automate, Power Apps, and Azure that makes AI Insights so powerful. Intelligence extends beyond the dashboard and permeates every data touchpoint.\nWhen Analysis Becomes Proactive and Intelligent\nPower BI’s AI Insights facilitate the shift from information to automated intuition. It’s not just about Data Visualization anymore but assisted decision-making. By integrating\npredictive models\n,\nintelligent analyses\n, and\ncognitive services\n, Power BI becomes a genuine\nanalytical assistant\ncapable of reading between the lines of your data and suggesting subsequent actions.\nHowever, knowing how to\ncapitalize on this intelligence\nis crucial: asking the right questions, verifying answers, and empowering teams to retain control over interpretation.\nIf you aim to fully master the\nAI features of Power BI\nand build smart data pipelines with Power Query, Power Automate, and Azure,\nDataScientest training\nis for you!\nOur Data Analyst course\ntrains you in\nall stages of data processing\n: modeling, visualization, automation, and introduction to decision-making AI. You will learn how to best utilize Power BI,\ncreate dynamic reports\n, and implement AutoML models to enrich your analyses!\nWe also offer\ncertification training\nspecifically dedicated to Microsoft solutions like\nPower BI\nor Microsoft Azure cloud. With our practice-oriented teaching, you will develop concrete skills to\nintegrate AI into your data projects\n.\nOur training courses\nare available in intensive BootCamp, continuing education, or work-study, and are eligible for CPF or France Travail funding.\nDiscover DataScientest\nand add intelligence to your data with artificial intelligence!\nDiscover our courses\nFor more insights on this topic, check out\nour comprehensive article dedicated to Power BI\nand our article focused on the Power Platform!"
    },
    {
        "title": "Gemini adds Temporary Chats and new personalization features",
        "link": "https://blog.google/products/gemini/temporary-chats-privacy-controls/",
        "date": "2025-08-13T16:00:00+00:00",
        "content": "Gemini adds Temporary Chats and new personalization features\n\nAug 13, 2025\n\n5 min read\n\nGeneral summary\n\n\n\nThe Gemini app is getting more personalized. Now Gemini can learn from past chats to give you better responses. Also, a new Temporary Chat feature lets you have conversations that won't be saved or used for personalization. Plus, you have more control over your data with updated settings.\n\n\n\nNew features in the Gemini app provide you with an even better personalized experience.\n\nToday, we’re updating the Gemini app so it becomes an even more personal, proactive and powerful assistant, while also providing you more control over your data.\n\nThe Gemini app can now reference your past chats to learn your preferences, delivering more personalized responses the more you use it. We’re also introducing a new privacy feature called Temporary Chats, and new settings that give you more control over your data. With these updates, you can create the experience that’s right for you and make the privacy choices that fit your needs.\n\nUse past chats to get more personalized responses\n\nAt I/O,we introduced our visionfor the Gemini app: to create an AI assistant that learns and truly understands you — not one that just responds to your prompt in the same way that it would anyone else’s prompt.\n\nToday, we’re introducing a new setting that allows Gemini to learn from your past conversations over time. When this setting is on, Gemini remembers key details and preferences you've shared, leading to more natural and relevant conversations, as if you're collaborating with a partner who's already up to speed.\n\nHere's how personal context can bring a conversation to life:\n\nWe’re rolling out this feature over the coming weeks, starting today. At first, personalized conversations will be available when using our 2.5 Pro model1inselect countriesand we plan to expand the feature to our 2.5 Flash model and more countries in the weeks ahead.\n\nThis setting is on by default to help Gemini give you more relevant responses, but you remain in control and can turn this setting on or off at any time. To do so, simply head to your Settings in the Gemini app and select “Personal context,” then ”Your past chats with Gemini.” As before, you canmanage and deleteyour conversations in Gemini Apps Activity.\n\nHave quick, one-off chats with Temporary Chat\n\nThere may be times when you want to have a quick conversation with the Gemini app without it influencing future chats. For example, you might be exploring private questions or simply brainstorming an idea that's outside your usual style. For these moments, we're also introducing a new feature calledTemporary Chat, which starts rolling out today and will expand to all users over the coming weeks.\n\nTemporary Chats won’t appear in yourrecent chatsor Gemini Apps Activity, and they won’t be used to personalize your Gemini experience or train Google’s AI models. They are kept for up to 72 hours to respond to you and to process any feedback you choose to provide.\n\nControl your data with new settings\n\nWe are continuously improving our models, products and services to make the Gemini app the most personal, proactive and powerful assistant. As part of this ongoing work, we’re updating how we handle the content you upload to Gemini, including files and photos. In the coming weeks, your \"Gemini Apps Activity\" setting will be renamed \"Keep Activity.\" When this setting is on, a sample of your future uploads will be used to help improve Google services for everyone.2If you prefer not to have your data used this way, you canturn this setting offor use Temporary Chats. If your Gemini Apps Activity setting is currently off, your Keep Activity setting will remain off, and you can turn it on anytime.\n\nEarlier this month, we also introduced a control that lets you choose whether the audio,video and screensyou share with Gemini through the mic button orGemini Liveare used to improve Google services for everyone. This setting is off by default, but you can turn it on anytime. Learn more about this setting in theGemini Apps Privacy Hub.\n\nBecause we know trust is earned through transparency and control, we believe it’s important to provide you with the tools to manage your data. You can adjust these settings whenever you like and learn more about how Google handles your data in ourGemini Apps Privacy Hub.\n\nGet more stories from Google in your inbox.Get morestories from Googlein your inbox.\n\nYour information will be used in accordance withGoogle's privacy policy.\n\nDone. Just one step more.\n\nCheck your inbox to confirm your subscription.\n\nYou are already subscribed to our newsletter.\n\nYou can also subscribe with adifferent email address.\n\nMore Information\n\nAvailable for over 18 consumer accounts only at this time.\n\nApplies to uploads submitted starting September 2, 2025.\n\nCollapse\n\nRelated stories\n\nCatch up on the newest features in August’s Gemini Drop.\n\nNew Gemini app tools to help students learn, understand and study even better\n\nGuided Learning in Gemini: From answers to understanding\n\nBringing the best of AI to college students for free\n\nCreate personal illustrated storybooks in the Gemini app.\n\nTry Deep Think in the Gemini app"
    },
    {
        "title": "Diffusion Models Demystified: Understanding the Tech Behind DALL-E and Midjourney",
        "link": "https://www.kdnuggets.com/diffusion-models-demystified-understanding-the-tech-behind-dall-e-and-midjourney",
        "date": "2025-08-13T00:00:00+00:00",
        "content": "Image by Author | Ideogram\n\n[Image: Diffusion Models Demystified: Understanding the Tech Behind DALL-E and Midjourney] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: Diffusion Models Demystified: Understanding the Tech Behind DALL-E and Midjourney] https://www.kdnuggets.com/wp-content/uploads/Diffusion-Models-Demystified-Understanding-the-Tech-Behind-DALLE-and-Midjourney_1.png\n\nGenerative AI models have emerged as a rising star in recent years, particularly with the introduction of large language model (LLM) products likeChatGPT. Using natural language that humans can understand, these models can process input and provide a suitable output. As a result of products like ChatGPT, other forms of generative AI have also become popular and mainstream.\n\nProducts such asDALL-EandMidjourneyhave become popular amid the generative AI boom due to their ability to generate images solely from natural language input. These popular products do not create images from nothing; instead, they rely on a model known as a diffusion model.\n\nIn this article, we will demystify the diffusion model to gain a deeper understanding of the technology behind it. We will discuss the fundamental concept, how the model works, and how it is trained.\n\nCurious? Let's get into it.\n\n#Diffusion Model Fundamentals\n\nDiffusion models are a class of AI algorithms that fall under the category of generative models, designed to generate new data based on training data. In the case of diffusion models, this means they can create new images from given inputs.\n\nHowever, diffusion models generate images through a different process than usual, where the model adds and then removes noise from data. In simpler terms, the diffusion model alters an image and then refines it to create the final product. You can think of the model as a denoising model, as it learns to remove noise from images.\n\nFormally, the diffusion model first emerged in the paperDeep Unsupervised Learning using Nonequilibrium Thermodynamicsby Sohl-Dickstein et al. (2015). The paper introduces the concept of converting data into noise using a process called the controlled forward diffusion process and then training a model to reverse the process and reconstruct the data, which is the denoising process.\n\nBuilding upon this foundation, the paperDenoising Diffusion Probabilistic Modelsby Ho et al. (2020) introduces the modern diffusion framework, which can produce high-quality images and outperform previous popular models, such as generative adversarial networks (GANs). In general, a diffusion model consists of two critical stages:\n\nForward (diffusion) process: Data is corrupted by incrementally adding noise until it becomes indistinguishable from random staticReverse (denoising) process: A neural network is trained to iteratively remove noise, learning how to reconstruct image data from complete randomness\n\nLet’s try to understand the diffusion model components better to have a clearer picture.\n\n//Forward Process\n\nThe forward process is the first phase, where an image is systematically degraded by adding noise until it becomes random static.\n\nThe forward process is controlled and iterative, which we can summarize in the following steps:\n\nStart with an image from the datasetAdd a small amount of noise to the imageRepeat this process many times (potentially hundreds or thousands), each time further corrupting the image\n\nAfter enough steps, the original image will appear as pure noise.\n\nThe process above is often modeled mathematically as a Markov chain, as each noisy version depends only on the one immediately preceding it, not on the entire sequence of steps.\n\nBut why should we gradually turn the image into noise instead of converting it straight into noise in one step? The goal is to enable the model to gradually learn how to reverse the corruption. Small, incremental steps allow the model to learn the transition from noisy to less-noisy data, which helps it reconstruct the image step-by-step from pure noise.\n\nTo determine how much noise is added at each step, the concept of a noise schedule is used. For example, linear schedules introduce noise steadily over time, whereas cosine schedules introduce noise more gradually and preserve useful image features for a more extended period.\n\nThat’s a quick summary of the forward process. Let’s learn about the reverse process.\n\n//Reverse Process\n\nThe next stage after the forward process is to turn the model into a generator, which learns to turn the noise back into image data. Through iterative small steps, the model can generate image data that previously did not exist.\n\nIn general, the reverse process is the inverse of the forward process:\n\nBegin with pure noise — an entirely random image composed of Gaussian noiseIteratively remove noise by using a trained model that tries to approximate a reverse version of each forward step. In each step, the model uses the current noisy image and the corresponding timestep as input, predicting how to reduce the noise based on what it learned during trainingStep-by-step, the image becomes progressively clearer, resulting in the final image data\n\nThis reverse process requires a model trained to denoise noisy images. Diffusion models often employ a neural network architecture, such as a U-Net, which is an autoencoder that combines convolutional layers in an encoder–decoder structure. During training, the model learns to predict the noise components added during the forward process. At each step, the model also considers the timestep, allowing it to adjust its predictions according to the level of noise.\n\nThe model is typically trained using a loss function such as mean squared error (MSE), which measures the difference between the predicted and actual noise. By minimizing this loss across many examples, the model gradually becomes proficient at reversing the diffusion process.\n\nCompared to alternatives like GANs, diffusion models offer more stability and a more straightforward generative path. The step-by-step denoising approach leads to more expressive learning, which makes training more reliable and interpretable.\n\nOnce the model is fully trained, generating a new image follows the reverse process we have summarized above.\n\n//Text Conditioning\n\nIn many text-to-image products, such as DALL-E and Midjourney, these systems can guide the reverse process using text prompts, which we refer to as text conditioning. By integrating natural language, we can acquire a matching scene rather than random visuals.\n\nThe process works by utilizing a pre-trained text encoder, such asCLIP (Contrastive Language–Image Pre-training), which converts the text prompt into a vector embedding. This embedding is then fed into the diffusion model architecture through a mechanism such as cross-attention, a type of attention mechanism that enables the model to focus on specific parts of the text and align the image generation process with the text. At each step of the reverse process, the model examines the current image state and the text prompt, utilizing cross-attention to align the image with the semantics from the prompt.\n\nThis is the core mechanism that allows DALL-E and Midjourney to generate images from prompts.\n\n#How Do DALL-E and Midjourney Differ?\n\nBoth products utilize diffusion models as their foundation but differ slightly in their technical applications.\n\nFor instance, DALL-E employs a diffusion model guided by CLIP-based embedding for text conditioning. In contrast, Midjourney features its proprietary diffusion model architecture, which reportedly includes a fine-tuned image decoder optimized for high realism.\n\nBoth models also rely on cross-attention, but their guidance styles differ. DALL-E emphasizes adhering to the prompt through classifier-free guidance, which balances between unconditioned and text-conditioned output. In contrast, Midjourney tends to prioritize stylistic interpretation, possibly employing a higher default guidance scale for classifier-free guidance.\n\nDALL-E and Midjourney differ in their handling of prompt length and complexity, as the DALL-E model can manage longer prompts by processing them before they enter the diffusion pipeline, while Midjourney tends to perform better with concise prompts.\n\nThere are more differences, but these are the ones you should know that relate to the diffusion models.\n\n#Conclusion\n\nDiffusion models have become a foundation of modern text-to-image systems such as DALL-E and Midjourney. By utilizing the foundational processes of forward and reverse diffusion, these models can generate entirely new images from randomness. Additionally, these models can use natural language to guide the results through mechanisms such as text conditioning and cross-attention.\n\nI hope this has helped!\n\nCornellius Yudha Wijayais a data science assistant manager and data writer. While working full-time at Allianz Indonesia, he loves to share Python and data tips via social media and writing media. Cornellius writes on a variety of AI and machine learning topics.\n\nMore On This Topic\n\nGenerative AI Playground: Text-to-Image Stable Diffusion with…Stable Diffusion: Basic Intuition Behind Generative AIDiffusion and Denoising: Explaining Text-to-Image Generative AI3 Ways to Generate Hyper-Realistic Faces Using Stable DiffusionUnveiling Midjourney 5.2: A Leap Forward in AI Image GenerationKick Ass Midjourney Prompts with Poe\n\n"
    },
    {
        "title": "10 GitHub Repositories to Master Frontend Development",
        "link": "https://www.kdnuggets.com/10-github-repositories-to-master-frontend-development",
        "date": "2025-08-13T00:00:00+00:00",
        "content": "Image by Author\n\n[Image: 10 GitHub Repositories to Master Frontend Development] data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='100%'%20height='0'%20viewBox='0%200%20100%%200'%3E%3C/svg%3E\n\n[Image: 10 GitHub Repositories to Master Frontend Development] https://www.kdnuggets.com/wp-content/uploads/awan_10_github_repositories_master_frontend_development_1.png\n\nIn today’s fast-evolving tech landscape, mastering frontend development is more important than ever. While AI can help automate parts of the coding process, building a polished, user-friendly web application still requires a solid understanding of frontend fundamentals. Many experienced developers find that relying solely on AI models like Claude 4 Sonnet can sometimes introduce more problems than it solves, especially when it comes to debugging or implementing complex design features. That’s why hands-on experience with frontend technologies remains crucial for anyone aiming to create effective, high-quality web applications.\n\nIn this article, we will explore 10 essential GitHub repositories packed with learning resources, tools, cheat sheets, guides, and project ideas. These repositories are carefully curated to help you learn, practice, and ultimately master frontend development in 2025.\n\n#Repositories to Master Frontend Development\n\n//1. Front-End Checklist: The Ultimate Launch-Ready Guide\n\nLink:thedaviddias/Front-End-Checklist\n\nThis comprehensive checklist ensures your website is production-ready, covering everything from HTML and CSS to performance, accessibility, SEO, and security. It’s a must-have for vibe coders who want to deliver high-quality, modern web projects.\n\n//2. Frontend Dev Bookmarks: Curated Resources for Every Topic\n\nLink:dypsilon/frontend-dev-bookmarks\n\nA collection of resources for frontend web developers, including algorithms, design patterns, animation, responsive design, accessibility, and more. This repository is a treasure trove for anyone looking to deepen their frontend knowledge .\n\n//3. Awesome Cheatsheets: Quick References for Every Tool\n\nLink:LeCoupa/awesome-cheatsheets\n\nA comprehensive collection of cheatsheets for popular programming languages, frameworks, and development tools. Ideal for quick lookups, it keeps essential syntax and commands at your fingertips. It includes frontend cheats for HTML, CSS, React.js, Vue.js, and more.\n\n//4. Full Stack FastAPI Template: Modern Web App Boilerplate\n\nLink:fastapi/full-stack-fastapi-template\n\nA production-ready template that combines FastAPI for the backend and React for the frontend, along with SQLModel, PostgreSQL, Docker, and GitHub Actions. This template is ideal for building robust, full-stack web applications with best practices integrated. It is particularly suitable for Python developers looking to incorporate frontend elements into their FastAPI applications.\n\n//5. Front-End Performance Checklist: Optimize for Speed\n\nLink:thedaviddias/Front-End-Performance-Checklist\n\nA detailed checklist focused on optimizing your website's performance covers everything from asset minification and lazy loading to caching strategies. This will help you deliver lightning-fast user experiences. If you are using AI to build your website, please use this repository to optimize its performance.\n\n//6. Frontend Challenges: Real-World Coding Practice\n\nLink:felipefialho/frontend-challenges\n\nA public list of open-source frontend challenges from companies around the world. These challenges are designed to help you practice your skills, build your portfolio, and prepare for job interviews. Highly recommended for final year students, job seekers, and anyone looking to advance their career in the field.\n\n//7. Frontend Stuff: Expanding List of Frameworks and Tools\n\nLink:moklick/frontend-stuff\n\nThis is a public list of open-source frontend challenges from companies worldwide. These challenges are intended to help you practice your skills, enhance your portfolio, and prepare for job interviews. They are highly recommended for final-year students, job seekers, and anyone looking to advance their career in the field.\n\n//8. Frontend Guidelines: Best Practices for HTML, CSS, and JS\n\nLink:bendc/frontend-guidelines\n\nA collection of best practices for writing clean, maintainable HTML, CSS, and JavaScript. This guide helps you follow industry standards and improve code quality across your projects. It is a cheat sheet, with examples.\n\n//9. Project Ideas and Resources: Inspiration for Your Next Build\n\nLink:The-Cool-Coders/Project-Ideas-And-Resources\n\nA collection of application ideas and resources to help you improve your coding skills. It is perfect for finding inspiration and challenging yourself with new frontend projects. The repository includes beginner, intermediate, expert, and specialized projects with guides.\n\n//10. Front-End Web Development Resources: Your Learning Companion\n\nLink:RitikPatni/Front-End-Web-Development-Resources\n\nA comprehensive repository of content and links to help you on your journey as a frontend web developer. Includes tutorials, articles, tools, roadmaps, and more for every stage of your learning.\n\nAbid Ali Awan(@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness.\n\nMore On This Topic\n\n10 GitHub Repositories to Master Web Development in 202510 GitHub Repositories to Master Backend Development10 GitHub Repositories to Master Machine Learning10 GitHub Repositories to Master MLOps10 GitHub Repositories to Master Computer Science10 GitHub Repositories to Master Data Engineering\n\n"
    },
    {
        "title": "TensorFlow Playground: Making Deep Learning Easy",
        "link": "https://datascientest.com/en/all-about-deep-learning-with-tensorflow-playground",
        "date": "2025-08-13T00:00:00+00:00",
        "content": "Deep learning is as fascinating as it is intimidating. With its equations, GPUs, and esoteric vocabulary, one might think a doctorate in mathematics is necessary to understand its logic. Yet the principle is simple: learn by example. To see it firsthand — literally — nothing beats the\nTensorFlow Playground\n.\nThis small online tool allows you to manipulate a neural network in real time, observe the reactions, and, most importantly, understand\nhow\nit learns. Just a few minutes can turn an abstract concept into a concrete experience.\nDeep learning in brief\nFor over a decade,\ndeep learning\nhas been prevalent in image recognition, automatic translation, and text synthesis. Yet, the foundational idea dates back to the 1950s: crudely mimicking the functioning of biological neurons. An\nartificial neuron\nreceives numerical inputs, weights them, optionally adds a bias, and applies an activation function. Positioned in successive\nlayers\n, these neurons gradually transform raw data into representations capable of separating, predicting, or generating.\nWhy “deep”? Because modern networks stack dozens, even hundreds of layers, each capturing more subtle abstraction than the previous one: from edges to patterns, from patterns to objects, then from objects to the entire scene. The whole is trained using an optimization method — often gradient descent — that adjusts the weights to minimize an error measured on a sample of annotated examples.\nMore about Deep Learning\nTensorFlow Playground: a browser lab\nOpen\nTensorFlow Playground\nand, without installing anything, a minimal network appears. On the left,\ncolored points\nrepresent the data; in the center,\ncircles\n(the neurons) are connected by\narrows\n(the weights); on the right, the\nhyperparameters\ncan be adjusted with a simple click: learning rate, activation function, regularization, batch size, etc. When you press\nTrain\n, each iteration updates the decision boundary in real-time.\nWhy is this tool so powerful for understanding?\nInstant visualization\n: the boundary evolves before your eyes, illustrating gradient descent far better than a static graph.\nSafety\n: no risk of erasing a disk or overheating a GPU.\nEasy sharing\n: all options are encoded in the URL; just copy it to share an exact configuration.\nNetwork anatomy from Playground\n1. The datasets\nPlayground offers four synthetic datasets: a\nlinearly separable\ncloud, two\nnon-linear\nsets (circle and “moons”), and the fundamental\nspiral\nnicknamed “the snail”. These two-dimensional data are simple enough to fit in a graph, yet rich enough to test the power of a deep network.\n2. The features\nBy default, only the coordinates\nx\nand\ny\nare used as inputs. However, you can enable other derived features:\nx²\n,\ny²\n,\nx·y\n,\nsin(x)\n, or\nsin(y)\n. These transformations allow the model to better capture complex patterns. For instance, a circular-shaped cloud becomes much easier to separate if you add\nx² + y²\nas information: the decision boundary can then become circular, even with a simple network.\n3. The architecture\nBelow the data, a slider lets you add layers and adjust the number of neurons. A network\nwithout a hidden layer\nis equivalent to linear regression: it only resolves linear separations. With\none layer\nof three neurons, the model already captures curves. Three layers of eight neurons tackle the spiral dataset, but increasing depth further risks overfitting — hence the importance of regularization.\n4. The hyperparameters\nThe\nlearning rate\ncontrols the magnitude of updates: too large, the loss oscillates; too small, the model stagnates. The\nactivation functions\n— ReLU, tanh, sigmoid — inject the necessary non-linearity; ReLU often converges faster, tanh sometimes appears more stable.\nL2 regularization\nadds a penalty on the weights to prevent the network from memorizing noise.\n5. Visualizing results\nOnce training is initiated, two elements should be monitored: the\ndecision boundary\n, which evolves visually in the plane, and the\nloss curve\nat the bottom right. The boundary shows how the network learns to separate the classes; the more it aligns with the shape of the data, the better the model’s comprehension. The loss curve indicates whether the error decreases — a good sign that learning is advancing.\nTraining for TensorFlow Playground\nTwo challenges to replicate\nAll exercise parameters below are already encoded in the links; just click to land on the described configuration.\nChallenge 1\n: First Steps\nLink:\nChallenge – First Steps\nStart the training: in a few seconds, the boundary begins to draw a separation into two distinct areas. Then try to reduce the learning rate and observe how the model learns more slowly. Also change the activation function, for example, switching from\ntanh\nto\nReLU\n: the speed and shape of convergence may vary, even if the task remains simple. It’s a good first exercise to become accustomed to the parameters without getting lost in complexity.\nChallenge 2\n: Spiral\nLink:\nSpiral\nIn this second exercise, the network must learn to classify a spiral-shaped dataset — a pattern known for its difficulty. The intentionally limited starting configuration (only the features\nx\nand\ny\n) forces you to experiment with the architecture and hyperparameters to succeed.\nStart the training: the boundary is chaotic at first. It’s up to you to find a combination of layers, neurons, activation function, or even regularization, that allows the network to follow the pattern curves. It’s a good way to see how depth or a small parameter change can make a significant difference.\nBonus difficulty:\nno adding derived features\n. Everything must rely on the model’s structure.\nMastering Deep Learning with TensorFlow Playground\nInsights from the Playground\nSpending roughly ten minutes in Playground teaches three fundamental lessons:\nThe network learns by adjusting\nits weights to reduce the error; gradient descent is simply an automated cycle of trial and error.\nNon-linearity\n— whether through features or activations — is crucial as soon as a straight line isn’t enough.\nHyperparameters matter\n: a poor learning rate or an oversized architecture can ruin training as surely as a bug in the code.\nThese observations are seen, not guessed: the moving image imprints in the mind what three pages of algebra summarize less clearly.\nTensorFlow Playground is not intended to produce industrial models, but to\nvisualize the essence of deep learning\n: the progressive transformation of a data space under the influence of iterative learning. By reducing the subject to colored points and a few buttons, the tool makes the mechanics accessible to anyone with a browser. From there, making the leap to\nKeras\nor\nPyTorch\nbecomes a straightforward interface change. So, open the page, play for a few minutes, adjust a parameter, observe the result, and watch the theory come to life. Machine learning, however complex, always starts with a first click on Train.\nDiscover our courses"
    }
]